# -*- coding: utf-8 -*-
"""Aula 16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kDGgK6CDSBT5aqqRbkSNPfZqWf1_wUPt

# Aula 16 - Autoencoders

## Fundamentos e Aplicações de Autoencoders

### O que são Autoencoders?
- **Definição:**
  - Redes neurais usadas para aprender representações de dados, geralmente para redução de dimensionalidade. Elas tentam capturar as características mais importantes dos dados de entrada, de modo a poder reconstruí-los com boa precisão.
- **Arquitetura:**
  - Composta por duas partes principais: codificador (encoder) e decodificador (decoder). O codificador transforma os dados de entrada em uma representação de menor dimensão (codificação latente), e o decodificador reconstrói os dados de entrada a partir dessa codificação.

### Componentes de um Autoencoder
- **Codificador (Encoder):**
  - Reduz a dimensão dos dados de entrada.
  - **Função de ativação comum:** ReLU (Rectified Linear Unit), que introduz não-linearidade ao modelo, permitindo que ele aprenda representações mais complexas.
  - **Estrutura típica:** Composta por camadas densas (fully connected layers), onde cada camada subsequente possui menos neurônios do que a anterior, forçando a rede a aprender uma representação comprimida dos dados.

- **Codificação Latente:**
  - Representação compacta e comprimida dos dados de entrada.
  - **Tamanho da camada latente:** Determinado pela necessidade específica da aplicação e pela quantidade de informação que deve ser retida. Deve ser um compromisso entre a capacidade de compressão e a precisão de reconstrução.

- **Decodificador (Decoder):**
  - Reconstrói os dados originais a partir da codificação latente.
  - **Função de ativação comum:** Sigmoid ou Tanh, que são frequentemente utilizadas nas camadas finais para garantir que os valores de saída estejam no mesmo intervalo que os dados de entrada.
  - **Estrutura típica:** Semelhante ao codificador, mas em ordem inversa. As camadas possuem um número crescente de neurônios, expandindo a codificação latente de volta ao formato original dos dados de entrada.

### Processo de Treinamento
- **Função de Custo:**
  - O objetivo do treinamento é minimizar o erro de reconstrução, que é a diferença entre os dados de entrada e os dados reconstruídos pelo autoencoder. Uma função de custo comum é o erro quadrático médio (MSE - Mean Squared Error).
  - **Erro de reconstrução:** Mede o quanto os dados de saída se desviam dos dados de entrada.

- **Algoritmo de Otimização:**
  - Uso do algoritmo de descida do gradiente (Gradient Descent) para ajustar os pesos da rede neural de forma a minimizar a função de custo.
  - **Learning rate:** Taxa de aprendizado que controla o tamanho dos passos dados na direção do gradiente negativo. Um learning rate adequado é crucial para garantir a convergência eficiente e estável do modelo.

### Aplicações de Autoencoders
- **Redução de Dimensionalidade:**
  - Extração de características principais dos dados de entrada, facilitando a visualização, compressão e análise dos dados.
  - Compressão de dados, permitindo armazenar grandes volumes de dados em um formato mais compacto.

- **Detecção de Anomalias:**
  - Identificação de dados que não se ajustam ao padrão normal, com base no erro de reconstrução. Dados que resultam em alto erro de reconstrução podem ser considerados anômalos.

- **Pré-treinamento para Redes Neurais Profundas:**
  - Inicialização de pesos para outras redes neurais, melhorando a eficiência e a eficácia do treinamento em tarefas subsequentes.

## Redução de Dimensionalidade de uma Tabela com Autoencoders

### Passos para Reduzir a Dimensionalidade de uma Tabela com Autoencoders

#### 1. Pré-processamento dos Dados
- **Normalização:** Escale seus dados para que todas as características estejam na mesma faixa de valores (por exemplo, entre 0 e 1).
  - Isso ajuda o modelo a convergir mais rapidamente e a melhorar a precisão da reconstrução.

#### 2. Definição do Modelo Autoencoder
- **Codificador (Encoder):**
  - Reduz a dimensão dos dados de entrada.
  - **Exemplo:** Se sua tabela original tem 100 características, você pode querer reduzir para 10 características principais.
- **Camada Latente:**
  - A camada de codificação compacta os dados para um espaço de dimensão menor.
  - **Exemplo:** Um vetor de 10 valores.
- **Decodificador (Decoder):**
  - Reconstrói os dados originais a partir da codificação latente.
  - A estrutura do decodificador deve ser simétrica ao codificador.

#### 3. Treinamento do Autoencoder
- **Função de Custo:**
  - Utilize uma função de custo como o erro quadrático médio (MSE) para medir a diferença entre os dados de entrada e os dados reconstruídos.
- **Algoritmo de Otimização:**
  - Utilize o algoritmo de descida do gradiente com uma taxa de aprendizado adequada.
- **Treinamento:**
  - Treine o autoencoder com seus dados até que a função de custo seja minimizada e a reconstrução seja suficientemente precisa.

#### 4. Extração das Características Latentes
- **Utilize apenas o Codificador:**
  - Após o treinamento, utilize apenas a parte do codificador do autoencoder para transformar seus dados de entrada na representação de baixa dimensionalidade.
- **Novo Conjunto de Dados:**
  - O resultado será uma nova tabela com o número de características reduzido, mas preservando a maior parte da informação essencial dos dados originais.
"""

import numpy as np
import pandas as pd
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

df = pd.read_csv('weather_data.csv')

df.head()

# Selecionar apenas as colunas numéricas para a normalização e o autoencoder
numeric_columns = ['Temperature_C', 'Humidity_pct', 'Precipitation_mm', 'Wind_Speed_kmh']
data = df[numeric_columns]

# Normalização dos dados
scaler = MinMaxScaler()
data_normalized = scaler.fit_transform(data)

# Definição do Autoencoder
input_dim = data_normalized.shape[1]
encoding_dim = 2  # Reduzir para 2 características principais

# Definindo a entrada do autoencoder
# input_layer define a camada de entrada com o formato dos dados de entrada (input_dim características)
input_layer = Input(shape=(input_dim,))

# Definindo a camada de codificação (encoder)
# encoder é uma camada densa com encoding_dim neurônios e ativação ReLU
# Esta camada reduz a dimensionalidade dos dados de entrada
encoder = Dense(encoding_dim, activation='relu')(input_layer)

# Definindo a camada de decodificação (decoder)
# decoder é uma camada densa com o mesmo número de neurônios que a camada de entrada e ativação sigmoid
# Esta camada reconstrói os dados para o formato original
decoder = Dense(input_dim, activation='sigmoid')(encoder)

# Construindo o modelo autoencoder
# autoencoder é o modelo que mapeia os dados de entrada para a saída reconstruída
autoencoder = Model(inputs=input_layer, outputs=decoder)

# Compilação do Autoencoder
# O compilador configura o modelo para o treinamento
# optimizer: algoritmo de otimização (Adam é um método de descida de gradiente estocástico)
# loss: função de perda (mean_squared_error calcula o erro quadrático médio entre a entrada e a saída reconstruída)
autoencoder.compile(optimizer='adam', loss='mean_squared_error')

# Treinamento do Autoencoder
autoencoder.fit(data_normalized, data_normalized, epochs=50, batch_size=256, shuffle=True)

# Extração da camada de codificação
encoder_model = Model(inputs=input_layer, outputs=encoder)
data_reduced = encoder_model.predict(data_normalized)

# Transformar a tabela reduzida em um DataFrame
df_reduced = pd.DataFrame(data_reduced, columns=['Feature1', 'Feature2'])

# Exibir as primeiras linhas da tabela reduzida
df_reduced.head()

# Para reconstruir as colunas originais a partir dos dados reduzidos:
# Definir o modelo de decodificação
encoded_input = Input(shape=(encoding_dim,))
decoder_layer = autoencoder.layers[-1]  # Última camada do autoencoder
decoder_model = Model(inputs=encoded_input, outputs=decoder_layer(encoded_input))

# Reconstruir os dados originais
data_reconstructed = decoder_model.predict(data_reduced)

# Reverter a normalização
data_reconstructed = scaler.inverse_transform(data_reconstructed)

# Transformar os dados reconstruídos em um DataFrame
df_reconstructed = pd.DataFrame(data_reconstructed, columns=numeric_columns)

# Exibir as primeiras linhas da tabela reconstruída
df_reconstructed.head()

# Exibe os dados originais
data.head()

"""OBS: Assim como em outras redes neurais, aqui também lidamos apenas com os dados numéricos"""

# Função para calcular o tamanho total em memória de um DataFrame
def memory_usage_df(df):
    return df.memory_usage(deep=True).sum() / (1024 * 1024)

# Tamanho em memória das variáveis
size_data = memory_usage_df(data)
size_df_reduced = memory_usage_df(df_reduced)
size_df_reconstructed = memory_usage_df(df_reconstructed)


print(f'Tamanho de "data" em memória: {size_data:.2f} MB')
print(f'Tamanho de "df_reduced" em memória: {size_df_reduced:.2f} MB')
print(f'Tamanho de "df_reconstructed" em memória: {size_df_reconstructed:.2f} MB')

"""# Comparação de DataFrames Usando Métricas de Reconstrução

## Função `compare_dataframes`

Esta função compara dois DataFrames, um original e um reconstruído, e calcula as métricas de qualidade de reconstrução para cada coluna. As métricas calculadas são:
- **Erro Quadrático Médio (MSE)**
- **Erro Absoluto Médio (MAE)**
- **Coeficiente de Determinação (R² Score)**

### Parâmetros
- `original_df` (DataFrame): O DataFrame original com os dados antes da redução de dimensionalidade.
- `reconstructed_df` (DataFrame): O DataFrame reconstruído a partir da representação de baixa dimensionalidade.

### Retorno
- DataFrame com as colunas:
  - `Column`: Nome da coluna.
  - `MSE`: Erro Quadrático Médio para a coluna.
  - `MAE`: Erro Absoluto Médio para a coluna.
  - `R²`: Coeficiente de Determinação para a coluna.
"""

def compare_dataframes(original_df, reconstructed_df):
    results = []

    for column in original_df.columns:
        mse = mean_squared_error(original_df[column], reconstructed_df[column])
        mae = mean_absolute_error(original_df[column], reconstructed_df[column])
        r2 = r2_score(original_df[column], reconstructed_df[column])

        results.append({
            'Column': column,
            'MSE': mse,
            'MAE': mae,
            'R²': r2
        })

    return pd.DataFrame(results)

# Aqui você deve ter os dataframes 'data' e 'df_reconstructed'
comparison_results = compare_dataframes(data, df_reconstructed)

# Exibir os resultados
print(comparison_results)

"""### Utilização de Autoencoders com Diferentes Tipos de Dados

#### Dados Numéricos Contínuos
- **Aplicação:** Autoencoders são altamente eficazes para dados numéricos contínuos, como medidas de temperatura, umidade, preços, etc.
- **Vantagens:** Redução de dimensionalidade, extração de características principais e compressão de dados.

#### Dados Numéricos Inteiros
- **Aplicação:** Autoencoders podem ser usados para dados inteiros que representam quantidades, como idade, contagem de itens, etc.
- **Considerações:** Normalização pode ser necessária para melhorar a performance do autoencoder.

#### Dados Numéricos que Representam Classes
- **Aplicação:** Dados inteiros que representam categorias (como níveis educacionais, códigos de produtos) podem não ser adequados para autoencoders diretamente.
- **Observação:** Nesses casos, técnicas de pré-processamento como one-hot encoding ou embeddings são recomendadas.

#### Dados Textuais
- **Aplicação:** Autoencoders não são diretamente aplicáveis a dados textuais brutos.
- **Observação:** Dados textuais precisam ser convertidos em representações numéricas (como embeddings) antes de serem usados com autoencoders.

### Conclusão
Autoencoders são ferramentas poderosas para a redução de dimensionalidade e extração de características, especialmente com dados numéricos contínuos. Para dados inteiros e categóricos, transformações adicionais podem ser necessárias. Dados textuais requerem etapas de pré-processamento para serem utilizados de forma eficaz com autoencoders.

# Exercícios com base de dados preço de casas na California
"""

df = pd.read_csv('housing.csv')
df.head()

df.info()

df.describe()

"""1 - Exclua as colunas 'Unnamed: 0', 'Latitude' e 'Longitude'.


"""

import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

df_cleaned = df.drop(columns=['Unnamed: 0', 'Latitude', 'Longitude'])

"""2 - Normalize a base de dados com MinMaxScaler.


"""

scaler = MinMaxScaler()

df_normalized = pd.DataFrame(scaler.fit_transform(df_cleaned), columns=df_cleaned.columns)

"""3 - Defina e compile um modelo de autoencoder para reduzir para 3 colunas (encoding_dim = 3).


"""

encoding_dim = 3

input_dim = df_normalized.shape[1]

input_layer = Input(shape=(input_dim,))
encoder = Dense(encoding_dim, activation='relu')(input_layer)
decoder = Dense(input_dim, activation='sigmoid')(encoder)

autoencoder = Model(inputs=input_layer, outputs=decoder)
autoencoder.compile(optimizer='adam', loss='mse')

"""4 - Treine o modelo por 50 épocas com batch_size de 64.


"""

autoencoder.fit(df_normalized, df_normalized, epochs=50, batch_size=64, shuffle=True)

"""5 - Faça a extração da camada de codificação (previsão com o modelo).


"""

encoder_model = Model(inputs=input_layer, outputs=encoder)

encoded_data = encoder_model.predict(df_normalized)

encoded_df = pd.DataFrame(encoded_data, columns=[f'encoded_{i+1}' for i in range(encoding_dim)])

"""6 - Crie um DataFrame reduzido com as 3 colunas.


"""

encoder_model = Model(inputs=autoencoder.input, outputs=autoencoder.get_layer(index=1).output)

encoded_data = encoder_model.predict(df_normalized)

encoded_df = pd.DataFrame(encoded_data, columns=[f'encoded_{i+1}' for i in range(encoding_dim)])

"""7 - Defina o decodificador para converter os dados reduzidos para o formato original.


"""

encoded_input = Input(shape=(encoding_dim,))
decoder_layer = autoencoder.layers[-1](encoded_input)
decoder_model = Model(inputs=encoded_input, outputs=decoder_layer)

"""8 - Reconstrua o DataFrame reduzido para a estrutura do original (usando as mesmas colunas).


"""

decoded_data = decoder_model.predict(encoded_df)

decoded_df = pd.DataFrame(decoded_data, columns=df_cleaned.columns)

"""9 - Exiba o tamanho em memória do DataFrame original, do DataFrame reduzido e do DataFrame reconstruído."""

original_size = df_cleaned.memory_usage(deep=True).sum()
reduced_size = encoded_df.memory_usage(deep=True).sum()
reconstructed_size = decoded_df.memory_usage(deep=True).sum()

original_size, reduced_size, reconstructed_size

"""10 - Exiba as métricas comparando o DataFrame original e o reconstruído."""

mse = mean_squared_error(df_cleaned, decoded_df)

mae = mean_absolute_error(df_cleaned, decoded_df)

r2 = r2_score(df_cleaned, decoded_df)

print(f"Erro Quadrático Médio (MSE): {mse}")
print(f"Erro Absoluto Médio (MAE): {mae}")
print(f"Coeficiente de Determinação (R²): {r2}")