{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XRibBj7CDEt"
      },
      "source": [
        "# Introdução\n",
        "---\n",
        "\n",
        "## Recapitulação Rápida\n",
        "\n",
        "- **Overfitting e Underfitting**: Temas recorrentes que já estudamos.\n",
        "- **Tipos de Aprendizagem**: Classificação e Regressão.\n",
        "- **Conceitos Avançados**: Risco empírico vs. risco estrutural, dimensão VC e dilema bias-variance.\n",
        "\n",
        "---\n",
        "\n",
        "## Objetivos da Aula de Hoje\n",
        "\n",
        "1. **Aprofundamento em Overfitting**: Entender o que ele realmente é, como identificá-lo e como podemos prevenir ou mitigar seus efeitos.\n",
        "    - Estaremos utilizando exemplos práticos para demonstrar esses conceitos.\n",
        "  \n",
        "2. **Teorema No Free Lunch**: Explorar o que significa este teorema e quais são suas implicações para a aprendizagem de máquinas.\n",
        "    - Compreender por que não existe um algoritmo que seja o melhor para todos os problemas.\n",
        "\n",
        "---\n",
        "\n",
        "### Por que é Importante?\n",
        "\n",
        "- **Complexidade dos Modelos**: A escolha da complexidade do modelo tem um grande impacto na qualidade das previsões.\n",
        "- **Seleção de Modelos**: O Teorema No Free Lunch nos lembra que não há uma única 'bala de prata' em aprendizado de máquina."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDj19-WBCDEv"
      },
      "source": [
        "# Breve Recapitulação de Overfitting e Underfitting\n",
        "---\n",
        "\n",
        "## O Que é Overfitting?\n",
        "\n",
        "- **Definição**: Quando um modelo aprende o 'ruído' nos dados de treinamento a ponto de afetar negativamente o desempenho em dados não vistos.\n",
        "- **Sintomas**:\n",
        "    - Alto desempenho nos dados de treino.\n",
        "    - Baixo desempenho nos dados de teste.\n",
        "  \n",
        "---\n",
        "  \n",
        "## O Que é Underfitting?\n",
        "\n",
        "- **Definição**: Quando um modelo é demasiado simples para captar as complexidades dos dados e, por isso, apresenta desempenho ruim tanto no treino quanto no teste.\n",
        "- **Sintomas**:\n",
        "    - Baixo desempenho nos dados de treino.\n",
        "    - Baixo desempenho nos dados de teste.\n",
        "\n",
        "---\n",
        "\n",
        "## Como Detectamos Estes Fenômenos?\n",
        "\n",
        "- **Curvas de Aprendizagem**: Gráficos que mostram o desempenho do modelo em relação ao tamanho do conjunto de treinamento.\n",
        "- **Métricas de Desempenho**: Utilizando métricas como acurácia, precisão, revocação, F1-score, etc.\n",
        "  \n",
        "---\n",
        "\n",
        "**Nota**: Já abordamos esses tópicos em detalhes nas aulas anteriores. O objetivo hoje é ir além e entender como lidar com o overfitting e explorar o Teorema 'No Free Lunch'.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8coy9wvCDEw"
      },
      "source": [
        "# Objetivos da Aula de Hoje\n",
        "---\n",
        "\n",
        "## Visão Geral\n",
        "\n",
        "Hoje, nosso foco será em duas áreas-chave:\n",
        "\n",
        "1. **Aprofundamento em Overfitting**:\n",
        "    - **Por que é crucial?**: O overfitting é um dos problemas mais comuns em machine learning e pode levar a resultados enganosos.\n",
        "    - **O que faremos?**: Vamos nos aprofundar na identificação, prevenção e mitigação do overfitting, utilizando exemplos práticos e técnicas específicas.\n",
        "    - **Métodos Abordados**: Regularização, Early Stopping, entre outros.\n",
        "  \n",
        "2. **Teorema No Free Lunch**:\n",
        "    - **Por que é crucial?**: Este teorema nos mostra que não existe um único algoritmo que seja ideal para todos os tipos de problemas.\n",
        "    - **O que faremos?**: Vamos entender a teoria por trás do teorema e suas implicações práticas em machine learning.\n",
        "    - **Implicações**: Escolha de algoritmos, tuning de hiperparâmetros, entre outros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK_JiPACCDEw"
      },
      "source": [
        "# Aprofundamento em Overfitting\n",
        "---\n",
        "\n",
        "## Como Identificar Overfitting com Métricas (3 minutos)\n",
        "\n",
        "- **Validação Cruzada**: Uma forma robusta de avaliar o desempenho do modelo em diferentes subconjuntos de dados.\n",
        "- **Conjunto de Validação**: Separe um conjunto de dados para validação durante o treinamento.\n",
        "- **Métricas de Desempenho**: Acompanhe métricas como Acurácia, F1-score, etc., em ambos os conjuntos (treino e validação).\n",
        "\n",
        "---\n",
        "\n",
        "## Regularização como uma Abordagem para Mitigar Overfitting (4 minutos)\n",
        "\n",
        "- **Definição**: Técnica que adiciona um termo de penalidade à função de custo.\n",
        "- **Tipos Comuns**:\n",
        "    1. **L1 Regularization**: Adiciona o valor absoluto dos pesos como termo de penalidade.\n",
        "    2. **L2 Regularization**: Adiciona o quadrado dos pesos como termo de penalidade.\n",
        "- **Hiperparâmetros**: O fator de regularização é um hiperparâmetro a ser ajustado.\n",
        "\n",
        "---\n",
        "\n",
        "## Exemplo: Overfitting em Redes Neurais e Técnicas para Mitigar (8 minutos)\n",
        "\n",
        "- **Dropout**: Técnica de desativar aleatoriamente alguns neurônios durante o treinamento.\n",
        "- **Early Stopping**: Monitorar o desempenho no conjunto de validação e parar o treinamento quando ele começar a degradar.\n",
        "- **Dados Adicionais**: Às vezes, simplesmente adicionando mais dados pode ajudar a mitigar o overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk2M6B6yCDEx"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZISOE8s-CDEy"
      },
      "source": [
        "- A função np.random.rand do NumPy gera números aleatórios uniformemente distribuídos no intervalo [0.0, 1.0].\n",
        "- Os números são gerados a partir de uma distribuição uniforme sobre este intervalo, o que significa que cada número tem a mesma probabilidade de ser escolhido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERHLnvbnCDEy"
      },
      "outputs": [],
      "source": [
        "# Carregando os dados\n",
        "df = pd.read_csv('notebooks (1).csv')\n",
        "df_ = df.sample(frac=1).reset_index(drop=True)\n",
        "df_ = df_[1000:3000]\n",
        "X = df_.drop(columns='valor').values\n",
        "y = df_[['valor']]\n",
        "\n",
        "# Dividindo os dados em conjuntos de treinamento (60%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "scaler_features = MinMaxScaler()\n",
        "scaler_target  = MinMaxScaler()\n",
        "\n",
        "# Normaliza e ajusta o escalonizador com os dados de X de treinamento\n",
        "X_train = scaler_features.fit_transform(X_train)\n",
        "# Normaliza e ajusta o escalonizador com os dados de y de treinamento\n",
        "y_train = scaler_target.fit_transform(y_train)\n",
        "\n",
        "# Ajusta os dados de X_temp\n",
        "X_temp = scaler_features.transform(X_temp)\n",
        "# Ajusta os dados de y_temp\n",
        "y_temp = scaler_target.transform(y_temp)\n",
        "\n",
        "# Separa os dados em X e y de validação e teste\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdQ0wi9ACDEy"
      },
      "source": [
        "# Entendendo a Regularização para Combater o Overfitting\n",
        "\n",
        "## O que é Regularização?\n",
        "- **O Que é Regularização?**: Regularização é uma técnica que adiciona um termo de penalidade à função de custo.\n",
        "- **Por que Usar Regularização?**: É útil para evitar que o modelo capture ruído nos dados de treinamento, reduzindo, assim, o overfitting.\n",
        "\n",
        "\n",
        "É como um professor que te pede para explicar o porquê da sua resposta em um teste, desencorajando você de apenas decorar as respostas. No aprendizado de máquina, adicionamos um 'termo de penalidade' para desencorajar o modelo de ajustar demais aos dados de treinamento.\n",
        "\n",
        "## Tipos de Regularização\n",
        "\n",
        "### L1 (Lasso)\n",
        "\n",
        "- **Como Funciona**: Imagine que você tem um time de futebol e alguns jogadores nunca tocam na bola. A L1 remove esses jogadores do time.\n",
        "- **Quando Usar**: Use L1 quando você suspeita que muitos recursos (ou variáveis) não ajudam a prever a resposta.\n",
        "\n",
        "### L2 (Ridge)\n",
        "\n",
        "- **Como Funciona**: Em vez de demitir jogadores, o técnico (L2) diz a todos para jogarem mais devagar (menor peso).\n",
        "- **Quando Usar**: Use quando todos os recursos parecem úteis e você quer que eles contribuam igualmente.\n",
        "\n",
        "### Elastic Net\n",
        "\n",
        "- **Como Funciona**: É como combinar os técnicos de L1 e L2 para gerenciar o time.\n",
        "- **Quando Usar**: Use quando você não tem certeza de qual técnica de regularização escolher.\n",
        "\n",
        "## Onde Aplicar Regularização?\n",
        "\n",
        "- **Camadas Iniciais**: Regularizar as primeiras camadas pode ser útil se você acha que as entradas (recursos) podem conter ruídos ou informações irrelevantes.\n",
        "- **Camadas do Meio**: Podem ser regularizadas para tornar o modelo mais simples e rápido.\n",
        "- **Camadas Finais**: Evite regularizar demais para não perder as características aprendidas.\n",
        "\n",
        "## Como Escolher a Força da Regularização?\n",
        "\n",
        "- Ajuste o 'termo de penalidade'. Se for muito alto, o modelo pode ficar simples demais e perder importantes padrões nos dados (underfitting).\n",
        "- Use técnicas como validação cruzada para encontrar o melhor ajuste.\n",
        "\n",
        "## Resumo\n",
        "\n",
        "- A regularização é uma técnica poderosa para tornar seu modelo mais generalizável e menos propenso a overfitting.\n",
        "- Escolher o tipo e a intensidade da regularização pode depender do seu conhecimento específico do problema e de técnicas de ajuste de hiperparâmetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npj-7TQaCDEy"
      },
      "source": [
        "# Entendendo o Dropout para Combater o Overfitting\n",
        "\n",
        "## O que é Dropout?\n",
        "- **O Que é Dropout?**: Dropout é uma técnica de regularização em redes neurais que \"desliga\" aleatoriamente um subconjunto de neurônios durante o treinamento.\n",
        "- **Por que Usar Dropout?**: O Dropout evita que qualquer neurônio se torne excessivamente especializado em memorizar ruídos dos dados de treinamento, o que contribui para combater o overfitting.\n",
        "\n",
        "É como um time de futebol onde alguns jogadores são aleatoriamente mandados para o banco durante o jogo para garantir que a equipe não dependa demais de um único jogador estrela. Isso torna o time como um todo mais robusto.\n",
        "\n",
        "## Como Funciona o Dropout?\n",
        "\n",
        "- **Implementação**: Durante cada iteração de treinamento, alguns neurônios são escolhidos aleatoriamente para serem \"desativados\". Isso significa que esses neurônios não participam do processo de treinamento para essa iteração específica.\n",
        "  \n",
        "- **Taxa de Dropout**: É o percentual de neurônios que você quer desativar em cada iteração. Por exemplo, uma taxa de 0.5 significa que 50% dos neurônios em uma camada são desativados.\n",
        "\n",
        "## Onde Aplicar Dropout?\n",
        "\n",
        "- **Camadas Iniciais**: Aplicar Dropout nas primeiras camadas pode ajudar se você acredita que os neurônios estão desenvolvendo dependências indesejadas nos dados de entrada.\n",
        "\n",
        "- **Camadas Ocultas Densas**: É mais comum aplicar Dropout nas camadas ocultas onde há uma alta densidade de neurônios. Isso aumenta as chances de overfitting, e o Dropout pode ajudar a mitigar isso.\n",
        "\n",
        "- **Camadas Finais**: Cuidado ao aplicar Dropout próximo à camada de saída, especialmente em tarefas que requerem alta precisão. Desativar neurônios aqui pode levar a predições imprecisas.\n",
        "\n",
        "\n",
        "## Quando Usar Dropout?\n",
        "\n",
        "- **Camadas Densas e Complexas**: Dropout é comumente usado em camadas que possuem muitos neurônios, como camadas densas.\n",
        "  \n",
        "- **Problemas com Overfitting**: Quando o modelo está muito bem ajustado aos dados de treinamento e não generaliza bem para dados novos.\n",
        "\n",
        "## Cuidados ao Usar Dropout\n",
        "\n",
        "- **Não use uma taxa muito alta**: Desativar muitos neurônios pode levar a underfitting.\n",
        "  \n",
        "- **Ajuste durante a Validação**: Sempre verifique o desempenho em um conjunto de validação para encontrar a taxa ideal.\n",
        "\n",
        "## Resumo\n",
        "\n",
        "- Dropout é uma técnica eficaz para evitar overfitting em redes neurais.\n",
        "- É como adicionar uma forma de \"incerteza\" ou \"ruído\" durante o treinamento, tornando o modelo mais robusto.\n",
        "- A escolha da taxa de dropout e onde aplicá-la são decisões cruciais que podem requerer experimentação.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aP3BIzr3CDEz"
      },
      "outputs": [],
      "source": [
        "# Funções para criar os modelos\n",
        "# Função para criar o modelo base\n",
        "def create_base_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(8, activation='relu'),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Função para criar o modelo com L1 e L2 (Elastic Net)\n",
        "def create_l1_l2_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
        "        Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
        "        Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Função para criar o modelo com Dropout\n",
        "def create_dropout_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dense(8, activation='relu'),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Função para criar o modelo com L1, L2 e Dropout\n",
        "def create_l1_l2_dropout_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
        "        Dropout(0.2),\n",
        "        Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
        "        Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0tsUbapCDEz"
      },
      "outputs": [],
      "source": [
        "# Inicia com uma forma de entrada específica\n",
        "input_shape = X_train.shape[1]\n",
        "\n",
        "# Criar, compilar e treinar os modelos\n",
        "model_base = create_base_model(input_shape)\n",
        "model_l1_l2 = create_l1_l2_model(input_shape)\n",
        "model_dropout = create_dropout_model(input_shape)\n",
        "model_l1_l2_dropout = create_l1_l2_dropout_model(input_shape)\n",
        "\n",
        "# Compilando e treinando os modelos\n",
        "optimizer1 = Adam(learning_rate=0.001)\n",
        "optimizer2 = Adam(learning_rate=0.001)\n",
        "optimizer3 = Adam(learning_rate=0.001)\n",
        "optimizer4 = Adam(learning_rate=0.001)\n",
        "\n",
        "model_base.compile(optimizer=optimizer1, loss='mse')\n",
        "history_base = model_base.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
        "\n",
        "model_l1_l2.compile(optimizer=optimizer2, loss='mse')\n",
        "history_l1_l2 = model_l1_l2.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
        "\n",
        "model_dropout.compile(optimizer=optimizer3, loss='mse')\n",
        "history_dropout = model_dropout.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
        "\n",
        "model_l1_l2_dropout.compile(optimizer=optimizer4, loss='mse')\n",
        "history_l1_l2_dropout = model_l1_l2_dropout.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7B8kG2OCDEz",
        "outputId": "ed234b7d-b964-4c9c-d619-b9c4071b9ad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 3ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n",
            "13/13 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "# Calculando os MSE para os modelos\n",
        "y_pred_base = model_base.predict(X_test)\n",
        "y_pred_l1_l2 = model_l1_l2.predict(X_test)\n",
        "y_pred_dropout = model_dropout.predict(X_test)\n",
        "y_pred_l1_l2_dropout = model_l1_l2_dropout.predict(X_test)\n",
        "\n",
        "mse_base = mean_squared_error(y_test, y_pred_base)\n",
        "mse_l1_l2 = mean_squared_error(y_test, y_pred_l1_l2)\n",
        "mse_dropout = mean_squared_error(y_test, y_pred_dropout)\n",
        "mse_l1_l2_dropout = mean_squared_error(y_test, y_pred_l1_l2_dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "jw0-TrV4CDEz",
        "outputId": "0b324e91-6799-48c6-b319-ac18a1fc9c04"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plotando os resultados\n",
        "labels = ['Base', 'L1_L2', 'Dropout', 'L1_L2_Dropout']\n",
        "mse_values = [mse_base, mse_l1_l2, mse_dropout, mse_l1_l2_dropout]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(labels, mse_values, color='skyblue')\n",
        "plt.xlabel('MSE')\n",
        "plt.ylabel('Model Type')\n",
        "plt.title('Comparação de MSE nos Modelos')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.ticklabel_format(style='plain', axis='x')  # Remove notação científica\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX0iQXFvCDE0"
      },
      "source": [
        "# Como Identificar Overfitting com Métricas\n",
        "---\n",
        "\n",
        "## Definindo o Problema\n",
        "\n",
        "- **Pergunta Principal**: Como saber se o modelo está se ajustando demais aos dados de treinamento?\n",
        "- **Consequência**: Se o modelo está com overfitting, ele terá um desempenho ruim em dados não vistos.\n",
        "\n",
        "---\n",
        "\n",
        "## Utilizando Métricas de Desempenho\n",
        "\n",
        "- **Treinamento vs Validação**: É crucial comparar as métricas de desempenho nos conjuntos de treinamento e validação.\n",
        "    1. **Acurácia**\n",
        "    2. **F1-score**\n",
        "    3. **Curva ROC-AUC**\n",
        "    \n",
        "- **Indicadores de Overfitting**:\n",
        "    - Acurácia alta no conjunto de treinamento, mas baixa no conjunto de validação.\n",
        "    - F1-score desproporcionalmente menor no conjunto de validação.\n",
        "    - Curva ROC-AUC demonstrando divergência entre treino e validação.\n",
        "\n",
        "---\n",
        "\n",
        "## Visualizando com Gráficos\n",
        "\n",
        "- **Plot de Métricas**: Gráficos de linha para acompanhar a evolução das métricas ao longo das épocas ou iterações.\n",
        "    - Eixo X: Épocas ou Iterações\n",
        "    - Eixo Y: Valor da Métrica\n",
        "\n",
        "**Nota**: Na próxima seção, vamos olhar para um exemplo prático que inclui esses gráficos.\n",
        "\n",
        "---\n",
        "\n",
        "Pronto para ver isso na prática? Vamos mergulhar no código a seguir!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlBwKA04CDE0"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics as sm\n",
        "\n",
        "def metricas_regressao(X_test, y_test, scaler_y, model):\n",
        "    \"\"\"\n",
        "    Avalia métricas de regressão para um modelo e conjunto de teste fornecidos.\n",
        "\n",
        "    Parâmetros:\n",
        "    - X_test: características do conjunto de teste.\n",
        "    - y_test: rótulos verdadeiros do conjunto de teste.\n",
        "    - scaler_y: scaler utilizado para normalizar a variável alvo.\n",
        "    - model: modelo treinado para fazer previsões.\n",
        "\n",
        "    Retorna:\n",
        "    Métricas de avaliação de regressão impressas.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Fazer previsões usando o modelo fornecido\n",
        "    predict = model.predict(X_test)\n",
        "    if scaler_y == 0:\n",
        "        real = y_test\n",
        "    else:\n",
        "    # 2. Inverter a transformação para obter os valores originais (não normalizados)\n",
        "        predict = scaler_y.inverse_transform(predict)\n",
        "        real = scaler_y.inverse_transform(y_test)\n",
        "    # 3. Calcular R2 e R2 ajustado\n",
        "    k = X_test.shape[1]  # número de características independentes\n",
        "    n = len(X_test)  # tamanho da amostra\n",
        "    r2 = sm.r2_score(real, predict)\n",
        "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)  # fórmula para R2 ajustado\n",
        "\n",
        "    # 4. Imprimir métricas\n",
        "    print('Root Mean Square Error:', round(np.sqrt(np.mean(np.array(predict) - np.array(real))**2), 2))\n",
        "    print('Mean Square Error:', round(sm.mean_squared_error(real, predict), 2))\n",
        "    print('Mean Absolut Error:', round(sm.mean_absolute_error(real, predict), 2))\n",
        "    print('Median Absolut Error:', round(sm.median_absolute_error(real, predict), 2))\n",
        "    print('Explain Variance Score:', round(sm.explained_variance_score(real, predict) * 100, 2))\n",
        "    print('R2 score:', round(sm.r2_score(real, predict) * 100, 2))\n",
        "    print('Adjusted R2 =', round(adj_r2, 3) * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cnJiOYiCDE0",
        "outputId": "fe1dddfa-af42-42f2-b75b-2caf073917f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13/13 [==============================] - 0s 2ms/step\n",
            "Root Mean Square Error: 205.13\n",
            "Mean Square Error: 8739551.75\n",
            "Mean Absolut Error: 2207.02\n",
            "Median Absolut Error: 1633.67\n",
            "Explain Variance Score: 75.41\n",
            "R2 score: 75.29\n",
            "Adjusted R2 = 74.4\n"
          ]
        }
      ],
      "source": [
        "metricas_regressao(X_test, y_test, scaler_target, model_dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VU72OqtlCDE0",
        "outputId": "b21759ec-2465-4993-ba86-9fcc9cbf366f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 0s 2ms/step\n",
            "Root Mean Square Error: 365.07\n",
            "Mean Square Error: 5334347.76\n",
            "Mean Absolut Error: 1733.01\n",
            "Median Absolut Error: 1274.22\n",
            "Explain Variance Score: 86.74\n",
            "R2 score: 86.4\n",
            "Adjusted R2 = 86.2\n"
          ]
        }
      ],
      "source": [
        "metricas_regressao(X_train, y_train, scaler_target, model_dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iHf9fK7CDE0"
      },
      "source": [
        "# Análise dos Erros do Modelo com 2000 Registros\n",
        "\n",
        "## Métricas de Desempenho\n",
        "\n",
        "- **Root Mean Square Error (RMSE)**\n",
        "    - Teste: 35.15\n",
        "    - Treinamento: 64.24\n",
        "- **Mean Square Error (MSE)**\n",
        "    - Teste: 8084403.99\n",
        "    - Treinamento: 4111220.34\n",
        "- **Mean Absolute Error (MAE)**\n",
        "    - Teste: 2155.01\n",
        "    - Treinamento: 1572.19\n",
        "- **Median Absolute Error**\n",
        "    - Teste: 1608.74\n",
        "    - Treinamento: 1237.49\n",
        "- **Explained Variance Score**\n",
        "    - Teste: 79.15%\n",
        "    - Treinamento: 88.91%\n",
        "- **R2 Score**\n",
        "    - Teste: 79.15%\n",
        "    - Treinamento: 88.9%\n",
        "- **Adjusted R2**\n",
        "    - Teste: 78.4%\n",
        "    - Treinamento: 88.8%\n",
        "\n",
        "## Interpretação das Métricas\n",
        "\n",
        "- **Root Mean Square Error (RMSE)**\n",
        "    - Representa a raiz quadrada da média dos erros quadráticos. Valores menores indicam melhor ajuste do modelo.\n",
        "- **Mean Square Error (MSE)**\n",
        "    - É a média dos erros quadráticos. Valores mais baixos são melhores, mas é mais sensível a outliers.\n",
        "- **Mean Absolute Error (MAE)**\n",
        "    - É a média dos erros absolutos. Fornece uma ideia de quão erradas são as previsões.\n",
        "- **Median Absolute Error**\n",
        "    - É a mediana dos erros absolutos. Menos sensível a outliers que o MAE.\n",
        "- **Explained Variance Score**\n",
        "    - Mede a proporção da variância do target que é explicada pelo modelo. Valores mais próximos de 100% são ideais.\n",
        "- **R2 Score**\n",
        "    - Mede o quanto do target é explicado pelas features. Quanto mais próximo de 100%, melhor.\n",
        "- **Adjusted R2**\n",
        "    - Semelhante ao R2, mas ajustado pelo número de preditores no modelo. É mais útil quando comparando modelos com diferentes números de preditores.\n",
        "\n",
        "## Análise de Overfitting\n",
        "\n",
        "- O modelo tem um desempenho significativamente melhor nos dados de treinamento em comparação com os dados de teste em quase todas as métricas.\n",
        "- A diferença entre o R2 Score de treinamento e teste é aproximadamente 9.75%, o que pode ser um indicador de que o modelo está sofrendo de algum grau de overfitting.\n",
        "- O RMSE para os dados de treinamento é aproximadamente 64, enquanto para os dados de teste é 35. A diferença notável também aponta para o overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMQzx-0jCDE0"
      },
      "outputs": [],
      "source": [
        "# Carregando os dados\n",
        "X = df.drop(columns='valor').values\n",
        "y = df[['valor']]\n",
        "\n",
        "# Dividindo os dados em conjuntos de treinamento (60%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "scaler_features = MinMaxScaler()\n",
        "scaler_target  = MinMaxScaler()\n",
        "\n",
        "# Normaliza e ajusta o escalonizador com os dados de X de treinamento\n",
        "X_train = scaler_features.fit_transform(X_train)\n",
        "# Normaliza e ajusta o escalonizador com os dados de y de treinamento\n",
        "y_train = scaler_target.fit_transform(y_train)\n",
        "\n",
        "# Ajusta os dados de X_temp\n",
        "X_temp = scaler_features.transform(X_temp)\n",
        "# Ajusta os dados de y_temp\n",
        "y_temp = scaler_target.transform(y_temp)\n",
        "\n",
        "# Separa os dados em X e y de validação e teste\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10PU_1AVCDE1"
      },
      "outputs": [],
      "source": [
        "# Inicia com uma forma de entrada específica\n",
        "input_shape = X_train.shape[1]\n",
        "\n",
        "# Criar, compilar e treinar o melhor modelo\n",
        "model_dropout = create_dropout_model(input_shape)\n",
        "\n",
        "# Compilando e treinando os modelos\n",
        "optimizer3 = Adam(learning_rate=0.001)\n",
        "model_dropout.compile(optimizer=optimizer3, loss='mse')\n",
        "history_dropout = model_dropout.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3m_DNglCDE1",
        "outputId": "df756f99-e590-42d9-8ca9-e88dffbb85bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 0s 1ms/step\n",
            "Root Mean Square Error: 45.52\n",
            "Mean Square Error: 7072594.28\n",
            "Mean Absolut Error: 2075.26\n",
            "Median Absolut Error: 1604.98\n",
            "Explain Variance Score: 81.05\n",
            "R2 score: 81.04\n",
            "Adjusted R2 = 80.9\n"
          ]
        }
      ],
      "source": [
        "metricas_regressao(X_test, y_test, scaler_target, model_dropout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCAmy1SOCDE1",
        "outputId": "4ed801c7-7f77-441a-93fe-8e805f45b5e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 0s 1ms/step\n",
            "Root Mean Square Error: 9.36\n",
            "Mean Square Error: 5100234.99\n",
            "Mean Absolut Error: 1752.75\n",
            "Median Absolut Error: 1351.06\n",
            "Explain Variance Score: 86.21\n",
            "R2 score: 86.21\n",
            "Adjusted R2 = 86.2\n"
          ]
        }
      ],
      "source": [
        "metricas_regressao(X_train, y_train, scaler_target, model_dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR9hm79SCDE1"
      },
      "source": [
        "# Análise dos Erros do Modelo com Acréscimo de Dados\n",
        "\n",
        "## Contexto\n",
        "\n",
        "- Este modelo foi treinado com um conjunto de 10,000 registros, um aumento significativo em relação aos experimentos anteriores que tinham menos registros.\n",
        "\n",
        "## Métricas de Desempenho e Análise\n",
        "\n",
        "### Root Mean Square Error (RMSE)\n",
        "- **Teste**: 127.68\n",
        "- **Treinamento**: 157.96\n",
        "  - Representa o desvio padrão dos erros do modelo. Valores menores indicam um melhor desempenho.\n",
        "\n",
        "### Mean Square Error (MSE)\n",
        "- **Teste**: 6769172.38\n",
        "- **Treinamento**: 5058889.6\n",
        "  - É a média dos erros ao quadrado, sendo sensível a outliers. Valores menores são melhores.\n",
        "\n",
        "### Mean Absolute Error (MAE)\n",
        "- **Teste**: 2017.72\n",
        "- **Treinamento**: 1749.72\n",
        "  - É a média dos erros absolutos, dando uma ideia da magnitude dos erros.\n",
        "\n",
        "### Median Absolute Error\n",
        "- **Teste**: 1544.37\n",
        "- **Treinamento**: 1353.14\n",
        "  - A mediana dos erros absolutos e é menos sensível a outliers.\n",
        "\n",
        "### Explained Variance Score\n",
        "- **Teste**: 81.9%\n",
        "- **Treinamento**: 86.39%\n",
        "  - Representa quanto da variância total é explicada pelo modelo.\n",
        "\n",
        "### R2 Score\n",
        "- **Teste**: 81.86%\n",
        "- **Treinamento**: 86.32%\n",
        "  - Indica o ajuste do modelo aos dados observados.\n",
        "\n",
        "### Adjusted R2\n",
        "- **Teste**: 81.7%\n",
        "- **Treinamento**: 86.3%\n",
        "  - É o R2 ajustado pelo número de preditores no modelo.\n",
        "\n",
        "## Efeito do Acréscimo de Dados\n",
        "\n",
        "- O acréscimo de mais dados no treinamento parece ter ajudado o modelo a generalizar melhor, como evidenciado pelas métricas de teste e treinamento mais próximas.\n",
        "- A diferença no R2 Score entre treinamento e teste diminuiu, sugerindo que o modelo está menos propenso a overfitting.\n",
        "- A inclusão de mais dados pode ter contribuído para uma representação mais abrangente do espaço de características, tornando o modelo mais robusto a variações nos dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmFCDsmGCDE1"
      },
      "source": [
        "# Teorema No Free Lunch\n",
        "---\n",
        "\n",
        "## Introdução\n",
        "\n",
        "- **Definição**: O Teorema No Free Lunch (NFL) afirma que não existe um único algoritmo de aprendizado de máquina que funcione melhor para todos os tipos de problemas.\n",
        "- **Importância**: Esse teorema nos ajuda a entender por que a busca pelo \"algoritmo perfeito\" é fútil.\n",
        "\n",
        "---\n",
        "\n",
        "## O Que o Teorema Realmente Significa?\n",
        "\n",
        "1. **Não Existe Algoritmo Universalmente Superior**: Cada algoritmo tem seus próprios pontos fortes e fracos, e o que funciona bem para um problema pode não ser adequado para outro.\n",
        "2. **Dependência do Problema**: O sucesso de um algoritmo é fortemente dependente do tipo de problema que você está tentando resolver.\n",
        "3. **A Importância da Experimentação**: Este teorema reforça a necessidade de experimentar com vários algoritmos e técnicas para encontrar a melhor abordagem para um determinado problema.\n",
        "\n",
        "---\n",
        "\n",
        "## Implicações Práticas\n",
        "\n",
        "- **Seleção de Modelos**: Dada a impossibilidade de um único melhor algoritmo, a seleção de modelos torna-se crucial.\n",
        "- **Otimização de Hiperparâmetros**: O ajuste de hiperparâmetros é mais relevante do que nunca, já que o \"melhor\" algoritmo é problema-específico.\n",
        "\n",
        "---\n",
        "\n",
        "**Exemplos práticos que ilustram o Teorema No Free Lunch em ação.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "Dit2z-5fCDE1",
        "outputId": "5f2a3731-cc25-4d87-b373-e467871cd55e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 3ms/step\n",
            "7/7 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Importando as bibliotecas necessárias\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Gerando um conjunto de dados de exemplo para regressão\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Dividindo os dados em conjuntos de treinamento, validação e teste\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Criando e treinando os modelos de RandomForest e SVM para regressão\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "svm = SVR()\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Função para criar o modelo de rede neural com L1 e L2\n",
        "def create_l1_l2_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
        "        Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
        "        Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Função para criar o modelo de rede neural com Dropout\n",
        "def create_dropout_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(16, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(8, activation='relu'),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Criando e treinando as redes neurais\n",
        "l1_l2_model = create_l1_l2_model(X_train.shape[1])\n",
        "dropout_model = create_dropout_model(X_train.shape[1])\n",
        "\n",
        "l1_l2_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
        "dropout_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
        "\n",
        "# Avaliando os modelos\n",
        "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
        "svm_mse = mean_squared_error(y_test, svm.predict(X_test))\n",
        "l1_l2_mse = mean_squared_error(y_test, l1_l2_model.predict(X_test).reshape(-1))\n",
        "dropout_mse = mean_squared_error(y_test, dropout_model.predict(X_test).reshape(-1))\n",
        "\n",
        "# Gráfico para comparar as métricas\n",
        "labels = ['Random Forest', 'SVM', 'L1_L2', 'Dropout']\n",
        "values = [rf_mse, svm_mse, l1_l2_mse, dropout_mse]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(labels, values, color=['blue', 'green', 'red', 'purple'])\n",
        "plt.xlabel('Modelos')\n",
        "plt.ylabel('Erro Quadrático Médio (MSE)')\n",
        "plt.title('Comparação de MSE entre Modelos')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn7E6uEfCDE2"
      },
      "source": [
        "# Significado e Importância\n",
        "---\n",
        "\n",
        "## O Que o Teorema No Free Lunch Realmente Significa?\n",
        "\n",
        "- **Sem Almoço Grátis**: O nome sugere que não existe uma \"refeição gratuita\" em termos de eficácia algorítmica; tudo tem um custo.\n",
        "- **Universalidade**: Não existe um único algoritmo que seja superior em todos os cenários e tipos de dados.\n",
        "  \n",
        "---\n",
        "\n",
        "## Por Que Isso é Importante?\n",
        "\n",
        "### Fim da Busca pelo \"Algoritmo de Aprendizado Perfeito\"\n",
        "\n",
        "- Elimina a noção de que poderia existir um \"Santo Graal\" dos algoritmos de aprendizado de máquina.\n",
        "  \n",
        "### Acentua a Necessidade de Personalização\n",
        "\n",
        "- Destaca que a escolha do algoritmo deve ser adaptada ao problema específico em mãos.\n",
        "\n",
        "---\n",
        "\n",
        "**Para resumir, o Teorema No Free Lunch nos ensina a ser mais críticos e adaptáveis como cientistas de dados.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRrMEseuCDE2"
      },
      "source": [
        "# Como o Teorema No Free Lunch se Relaciona com Overfitting\n",
        "---\n",
        "\n",
        "## Introdução\n",
        "\n",
        "- **Ponto em Comum**: Tanto o Teorema No Free Lunch quanto o conceito de overfitting nos dizem que não há soluções universais no campo do aprendizado de máquina.\n",
        "\n",
        "---\n",
        "\n",
        "## Relação entre NFL e Overfitting\n",
        "\n",
        "1. **Escolha de Algoritmos**: O Teorema No Free Lunch sugere que devemos escolher algoritmos com base no problema específico, algo que também é crucial para evitar o overfitting.\n",
        "\n",
        "2. **Complexidade do Modelo**:\n",
        "    - NFL nos adverte contra a busca por um \"algoritmo perfeito\".\n",
        "    - Overfitting nos adverte contra a busca por um \"modelo perfeitamente ajustado\".\n",
        "    - Ambos são contra a ideia de \"um tamanho serve para todos\".\n",
        "\n",
        "3. **Personalização e Ajuste**:\n",
        "    - NFL destaca a necessidade de ajuste e personalização nos algoritmos.\n",
        "    - Overfitting nos mostra que esse ajuste precisa ser feito com cuidado para evitar memorização em vez de generalização.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusão\n",
        "\n",
        "- A consciência do Teorema No Free Lunch pode nos ajudar a ser mais criteriosos na prevenção de overfitting, escolhendo e ajustando algoritmos de forma mais eficaz.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKxWYWd0CDE2"
      },
      "source": [
        "# Implicações Práticas para a Seleção de Modelos\n",
        "---\n",
        "\n",
        "## Introdução\n",
        "\n",
        "- **Contexto**: Agora que entendemos os conceitos de overfitting e o Teorema No Free Lunch, como aplicamos esse conhecimento na prática?\n",
        "\n",
        "---\n",
        "\n",
        "## Passos na Seleção de Modelos\n",
        "\n",
        "1. **Análise do Problema**:\n",
        "    - Entender o tipo de problema (Classificação, Regressão, Agrupamento, etc.) é o primeiro passo na seleção do modelo.\n",
        "\n",
        "2. **Teste de Vários Modelos**:\n",
        "    - Devido ao Teorema No Free Lunch, é recomendável testar diversos algoritmos para encontrar o que se adequa melhor ao problema específico.\n",
        "\n",
        "3. **Validação Cruzada**:\n",
        "    - Usar técnicas como K-Fold para estimar o desempenho do modelo em dados não vistos e ajudar a evitar overfitting.\n",
        "\n",
        "4. **Ajuste de Hiperparâmetros**:\n",
        "    - Com o modelo escolhido, o ajuste de hiperparâmetros torna-se crucial tanto para o desempenho quanto para evitar o overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "## Métricas e Diagnósticos\n",
        "\n",
        "- **Selecionar Métricas Relevantes**: Acurácia, Precisão, Recall, F1-Score, entre outros, dependendo do problema.\n",
        "- **Curvas ROC e AUC**: Ferramentas úteis para avaliar a qualidade do modelo em problemas de classificação.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusão\n",
        "\n",
        "- A integração desses conceitos (NFL e overfitting) nos dá uma abordagem mais robusta e informada para a seleção e otimização de modelos em aprendizado de máquina.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjBszmsZCDE2"
      },
      "source": [
        "# Exercícios\n",
        "\n",
        "Exemplos de aplicação de l1, l2 e l1 e l2 juntos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5OmkN7ECDE2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Importando as bibliotecas necessárias para o modelo de rede neural\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "\n",
        "# Função para criar o modelo com regularização L1\n",
        "def create_l1_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Função para criar o modelo com regularização L2\n",
        "def create_l2_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Função para criar o modelo com regularização L1 e L2\n",
        "def create_l1_l2_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
        "        Dense(1, activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MerTz5pPCDE2"
      },
      "source": [
        "### Estrutura modelo base 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJJ5RueaCDE3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Função para criar um modelo com 7 camadas ocultas\n",
        "def create_7_layer_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(512, activation='relu', input_shape=(input_shape,)), # Camada de entrada\n",
        "        Dense(256, activation='relu'),  # Primeira camada oculta\n",
        "        Dense(128, activation='relu'),  # Segunda camada oculta\n",
        "        Dense(64, activation='relu'),   # Terceira camada oculta\n",
        "        Dense(32, activation='relu'),   # Quarta camada oculta\n",
        "        Dense(16, activation='relu'),   # Quinta camada oculta\n",
        "        Dense(8, activation='relu'),    # Sexta camada oculta\n",
        "        Dense(4, activation='relu'),    # Sétima camada oculta\n",
        "        Dense(1, activation='linear')   # Camada de saída\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Função para criar um modelo com 3 camadas ocultas\n",
        "def create_3_layer_model(input_shape):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_shape,)),  # Camada de entrada\n",
        "        Dense(32, activation='relu'),  # Primeira camada oculta\n",
        "        Dense(16, activation='relu'),  # Segunda camada oculta\n",
        "        Dense(8, activation='relu'),   # Terceira camada oculta\n",
        "        Dense(1, activation='linear')  # Camada de saída\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGYm3KrNCDE3"
      },
      "source": [
        "### Estrutura modelo base 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1JZkw0TCDE3"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(input_shape,)),  # Camada de entrada\n",
        "        Dense(32, activation='relu'),  # Primeira camada oculta\n",
        "        Dense(16, activation='relu'),  # Segunda camada oculta\n",
        "        Dense(8, activation='relu'),   # Terceira camada oculta\n",
        "        Dense(1, activation='linear')  # Camada de saída\n",
        "    ])\n",
        "model.compile(optimizer='adam', loss='mse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9s-BWGGCDE3"
      },
      "source": [
        "# Utilize a base de dados notebooks_ruidosos.csv que contém 3 colunas com valores aleatórios que devem atrapalhar o aprendizado do modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9q4YtClCDE3"
      },
      "source": [
        "\n",
        "## 1 - Separe os dados em X_train, X_temp, y_train, y_temp, com 50% para o treinamento normalize com fit transform os dados de treinamento (um normalizador para X e outro para y), normalize os dados temporários com o transform e em seguida separe X e y temp em X e y de validação e teste, 50% para cada um. (a coluna valor será o y)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ],
      "metadata": {
        "id": "Dqx-n7mLDDob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZZpPNOWCDE3"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('notebooks_ruidoso.csv')\n",
        "\n",
        "X = data.drop(columns=['valor'])\n",
        "y = data['valor']\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train = scaler_X.fit_transform(X_train)\n",
        "y_train = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "X_temp = scaler_X.transform(X_temp)\n",
        "y_temp = scaler_y.transform(y_temp.values.reshape(-1, 1))\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "ayjBRnAg2WhS",
        "outputId": "9d0a9ee5-5b69-48e6-c711-a4ef26d9df32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   marca   ano  qtd_nucleos  qtd_threads  ram  gpu_dedicada  ram_gpu  \\\n",
              "0      3  2021            8           15   16             1        4   \n",
              "1      8  2010            4            4   16             0        0   \n",
              "2      6  2013           12           12   16             0        0   \n",
              "3      2  2023            4            4   12             0        0   \n",
              "4      0  2010           12           16   16             0        0   \n",
              "\n",
              "   entradas_usb  duracao_bateria  resolucao_tela  tipo_tela  tela_touch  \\\n",
              "0             4              145               3          1           0   \n",
              "1             3              441               1          2           0   \n",
              "2             3              526               1          2           0   \n",
              "3             1               34               2          1           0   \n",
              "4             3              550               1          2           0   \n",
              "\n",
              "   armazenamento_hdd  armazenamento_ssd  aleatoria_1  aleatoria_2  \\\n",
              "0                  0                250          192           19   \n",
              "1               1000                250          131           29   \n",
              "2                500                250          190           44   \n",
              "3                  0                500          100           24   \n",
              "4               1000                250          157           48   \n",
              "\n",
              "   aleatoria_3  valor  \n",
              "0         1394  11791  \n",
              "1         1321  20430  \n",
              "2         1443  25962  \n",
              "3         1030   3156  \n",
              "4         2201  24619  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-137c6dd8-0334-4f36-a4a4-e851709ce004\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>marca</th>\n",
              "      <th>ano</th>\n",
              "      <th>qtd_nucleos</th>\n",
              "      <th>qtd_threads</th>\n",
              "      <th>ram</th>\n",
              "      <th>gpu_dedicada</th>\n",
              "      <th>ram_gpu</th>\n",
              "      <th>entradas_usb</th>\n",
              "      <th>duracao_bateria</th>\n",
              "      <th>resolucao_tela</th>\n",
              "      <th>tipo_tela</th>\n",
              "      <th>tela_touch</th>\n",
              "      <th>armazenamento_hdd</th>\n",
              "      <th>armazenamento_ssd</th>\n",
              "      <th>aleatoria_1</th>\n",
              "      <th>aleatoria_2</th>\n",
              "      <th>aleatoria_3</th>\n",
              "      <th>valor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>2021</td>\n",
              "      <td>8</td>\n",
              "      <td>15</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>145</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>250</td>\n",
              "      <td>192</td>\n",
              "      <td>19</td>\n",
              "      <td>1394</td>\n",
              "      <td>11791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>2010</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>441</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1000</td>\n",
              "      <td>250</td>\n",
              "      <td>131</td>\n",
              "      <td>29</td>\n",
              "      <td>1321</td>\n",
              "      <td>20430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>2013</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>526</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>500</td>\n",
              "      <td>250</td>\n",
              "      <td>190</td>\n",
              "      <td>44</td>\n",
              "      <td>1443</td>\n",
              "      <td>25962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>2023</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>500</td>\n",
              "      <td>100</td>\n",
              "      <td>24</td>\n",
              "      <td>1030</td>\n",
              "      <td>3156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>2010</td>\n",
              "      <td>12</td>\n",
              "      <td>16</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>550</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1000</td>\n",
              "      <td>250</td>\n",
              "      <td>157</td>\n",
              "      <td>48</td>\n",
              "      <td>2201</td>\n",
              "      <td>24619</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-137c6dd8-0334-4f36-a4a4-e851709ce004')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-137c6dd8-0334-4f36-a4a4-e851709ce004 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-137c6dd8-0334-4f36-a4a4-e851709ce004');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4b5ebf39-27d1-486f-8fa5-84b97bb1eb58\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4b5ebf39-27d1-486f-8fa5-84b97bb1eb58')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4b5ebf39-27d1-486f-8fa5-84b97bb1eb58 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JpJdIUSCDE3"
      },
      "source": [
        "\n",
        "## 2 - De acordo com as peculiaridades de aplicação da regularização de kernel L1, L2 e Elastic Net (L1 e L2 Juntos), em seguida crie uma função que gere um modelo baseado na estrutura do modelo de 7 camadas ocultas que melhor se adeque para a base de dados (de acordo com a teoria), crie e treine por 50 épocas um modelo gerado com esta função;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snDAPKyHCDE3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15728a30-ce59-4a22-f423-c83ba133291a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 3s 4ms/step - loss: 78931208.0000 - val_loss: 11515894.0000\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 10823921.0000 - val_loss: 9893282.0000\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 9776771.0000 - val_loss: 9846878.0000\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 9348626.0000 - val_loss: 9089190.0000\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 8976646.0000 - val_loss: 8860431.0000\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 8728686.0000 - val_loss: 8568049.0000\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8432791.0000 - val_loss: 8537464.0000\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8237634.0000 - val_loss: 8299958.0000\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8072759.5000 - val_loss: 8023151.0000\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7835975.5000 - val_loss: 7879564.5000\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7743984.0000 - val_loss: 7941786.0000\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7576602.0000 - val_loss: 8223333.0000\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 7462716.5000 - val_loss: 7543094.5000\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7339567.0000 - val_loss: 7428059.5000\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7129524.5000 - val_loss: 7885448.5000\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7198035.0000 - val_loss: 7437950.0000\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7012109.0000 - val_loss: 7036719.5000\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 6902009.0000 - val_loss: 7027670.0000\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 6855867.0000 - val_loss: 7030559.5000\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 6834953.0000 - val_loss: 7281763.5000\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6810161.0000 - val_loss: 7374219.5000\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6718772.0000 - val_loss: 6801685.0000\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6695670.5000 - val_loss: 6973817.0000\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6624561.5000 - val_loss: 6733032.0000\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6561655.0000 - val_loss: 7004471.5000\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6483863.0000 - val_loss: 6802211.5000\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6596689.0000 - val_loss: 6927619.5000\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 6454292.5000 - val_loss: 6799970.5000\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6470297.0000 - val_loss: 6782617.0000\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6509551.5000 - val_loss: 6882579.5000\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6392932.5000 - val_loss: 6856648.0000\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 6450860.5000 - val_loss: 6706381.0000\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 6390447.0000 - val_loss: 7506152.0000\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 6391184.5000 - val_loss: 7124563.5000\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 6346665.5000 - val_loss: 6894303.0000\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 6355834.5000 - val_loss: 6661006.0000\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 6237337.0000 - val_loss: 6716684.5000\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 6375450.5000 - val_loss: 6660734.0000\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 6339586.0000 - val_loss: 6687092.0000\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6293309.0000 - val_loss: 6871355.5000\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 6233467.5000 - val_loss: 6835197.5000\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 6217352.5000 - val_loss: 6825490.5000\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 6237974.0000 - val_loss: 6939614.0000\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 6277604.5000 - val_loss: 6745550.5000\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6277665.5000 - val_loss: 6765029.5000\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6157519.0000 - val_loss: 6780809.0000\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6248099.0000 - val_loss: 6698713.0000\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 6160375.5000 - val_loss: 7026184.0000\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 6155344.5000 - val_loss: 6750590.0000\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 6103172.5000 - val_loss: 6734092.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x797cb2e97be0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "X = data.drop('valor', axis=1)\n",
        "y = data['valor']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "def create_regularized_model(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "    return model\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = 1\n",
        "model = create_regularized_model(input_dim, output_dim)\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rdgH6hhCDE4"
      },
      "source": [
        "\n",
        "## 3 - De acordo com as peculiaridades de aplicação da aplicação do dropout, crie uma função que gere um modelo baseado na estrutura do modelo de 7 camadas ocultas que melhor se adeque para a base de dados (de acordo com a teoria), em seguida crie e treine por 50 épocas um modelo gerado com esta função;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrD54BgBCDE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a7d1f6-a655-4166-a5e3-8b5ef80364ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 4s 4ms/step - loss: 94458400.0000 - val_loss: 12883607.0000\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 18954992.0000 - val_loss: 11471315.0000\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 17739140.0000 - val_loss: 10260282.0000\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 17011032.0000 - val_loss: 9746019.0000\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 16387002.0000 - val_loss: 9428556.0000\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 15648319.0000 - val_loss: 9264579.0000\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14925315.0000 - val_loss: 9573418.0000\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 14830296.0000 - val_loss: 9340998.0000\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 14727225.0000 - val_loss: 11774252.0000\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14596139.0000 - val_loss: 10102126.0000\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14333882.0000 - val_loss: 11360161.0000\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14253572.0000 - val_loss: 11214787.0000\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14343311.0000 - val_loss: 9728646.0000\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 13879793.0000 - val_loss: 10029642.0000\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 13773302.0000 - val_loss: 9590338.0000\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13477418.0000 - val_loss: 10241604.0000\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13816407.0000 - val_loss: 9725162.0000\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13656080.0000 - val_loss: 10202332.0000\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13560321.0000 - val_loss: 9785437.0000\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13217479.0000 - val_loss: 10031182.0000\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13709970.0000 - val_loss: 11990225.0000\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13574680.0000 - val_loss: 15645937.0000\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13238675.0000 - val_loss: 12431029.0000\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13278560.0000 - val_loss: 10853553.0000\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13209253.0000 - val_loss: 11309139.0000\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13243354.0000 - val_loss: 12185047.0000\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12855016.0000 - val_loss: 8388055.5000\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12934521.0000 - val_loss: 8801791.0000\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12834778.0000 - val_loss: 11975771.0000\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12776184.0000 - val_loss: 10611554.0000\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13118171.0000 - val_loss: 9595589.0000\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12912892.0000 - val_loss: 9449196.0000\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12543725.0000 - val_loss: 9675107.0000\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12449913.0000 - val_loss: 12786109.0000\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 12275209.0000 - val_loss: 11316537.0000\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 12461476.0000 - val_loss: 10032367.0000\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12593874.0000 - val_loss: 9176843.0000\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 12506953.0000 - val_loss: 13376603.0000\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 12302848.0000 - val_loss: 11546553.0000\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 12063171.0000 - val_loss: 11567531.0000\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12314242.0000 - val_loss: 12482688.0000\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 12414347.0000 - val_loss: 11654713.0000\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12121344.0000 - val_loss: 10732932.0000\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 12362733.0000 - val_loss: 12965256.0000\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 11965747.0000 - val_loss: 11106985.0000\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 12460339.0000 - val_loss: 13823413.0000\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 12366797.0000 - val_loss: 12782497.0000\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 12081753.0000 - val_loss: 10855971.0000\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 12055837.0000 - val_loss: 12213913.0000\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 11858339.0000 - val_loss: 14233335.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x797c4b4c2950>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def create_model_with_dropout(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "    return model\n",
        "\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = 1\n",
        "\n",
        "model_with_dropout = create_model_with_dropout(input_dim, output_dim)\n",
        "\n",
        "model_with_dropout.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "model_with_dropout.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYoubnRYCDE4"
      },
      "source": [
        "\n",
        "## 4 - De acordo com as peculiaridades de aplicação da regularização de kernel L1, L2 e Elastic Net (L1 e L2 Juntos) e da aplicação do dropout, crie uma função que gere um modelo  baseado na estrutura do modelo de 7 camadas ocultas que melhor se adeque para a base de dados (basicamente mesclar os dois modelos anteriores), em seguida crie e treine por 50 épocas um modelo gerado com esta função;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqwngCrbCDE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ce88f2-b75a-4a18-bf78-d636d4844c47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 4s 5ms/step - loss: 88948096.0000 - val_loss: 12378746.0000\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 18899030.0000 - val_loss: 11174708.0000\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 17943760.0000 - val_loss: 10252235.0000\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 16802182.0000 - val_loss: 9645743.0000\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 16491373.0000 - val_loss: 9110335.0000\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 15721271.0000 - val_loss: 9367246.0000\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 15582327.0000 - val_loss: 9524227.0000\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 15641437.0000 - val_loss: 8919439.0000\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 15005259.0000 - val_loss: 9349434.0000\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 15232016.0000 - val_loss: 8616508.0000\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 15031340.0000 - val_loss: 9919749.0000\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 14746800.0000 - val_loss: 8647727.0000\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14649369.0000 - val_loss: 10931449.0000\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14378743.0000 - val_loss: 9525698.0000\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14509668.0000 - val_loss: 8971848.0000\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14067994.0000 - val_loss: 8381007.5000\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14586547.0000 - val_loss: 8397362.0000\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13921226.0000 - val_loss: 8631470.0000\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14255683.0000 - val_loss: 8273444.5000\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13935732.0000 - val_loss: 8973002.0000\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13916286.0000 - val_loss: 8105459.0000\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13601471.0000 - val_loss: 8401105.0000\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 13506961.0000 - val_loss: 9111550.0000\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 13990983.0000 - val_loss: 8222989.0000\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 13707054.0000 - val_loss: 8014591.0000\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14218870.0000 - val_loss: 8284656.0000\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13874394.0000 - val_loss: 8412006.0000\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13637433.0000 - val_loss: 9417996.0000\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13567564.0000 - val_loss: 9492242.0000\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13489969.0000 - val_loss: 8190887.0000\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13977504.0000 - val_loss: 8360550.0000\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13617186.0000 - val_loss: 8016656.5000\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13556231.0000 - val_loss: 9462624.0000\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13387333.0000 - val_loss: 8538397.0000\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13504453.0000 - val_loss: 8682277.0000\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 13624934.0000 - val_loss: 9662940.0000\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 13346091.0000 - val_loss: 7958137.5000\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13397486.0000 - val_loss: 8984641.0000\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13459758.0000 - val_loss: 7972620.0000\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13484990.0000 - val_loss: 7948234.0000\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13202874.0000 - val_loss: 8173586.0000\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13494453.0000 - val_loss: 9734215.0000\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13338271.0000 - val_loss: 9538799.0000\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12780091.0000 - val_loss: 9821676.0000\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13219964.0000 - val_loss: 8571185.0000\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12945318.0000 - val_loss: 9758120.0000\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 13170815.0000 - val_loss: 9111155.0000\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 12828838.0000 - val_loss: 9635480.0000\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 12866441.0000 - val_loss: 9094269.0000\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 3s 11ms/step - loss: 12775052.0000 - val_loss: 9240290.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x797c5daab700>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "def create_combined_model(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "\n",
        "    return model\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = 1\n",
        "\n",
        "combined_model = create_combined_model(input_dim, output_dim)\n",
        "\n",
        "combined_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "combined_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKIYW6DiCDE5"
      },
      "source": [
        "\n",
        "## 5 - De acordo com as peculiaridades de aplicação da regularização de kernel L1, L2 e Elastic Net (L1 e L2 Juntos), em seguida crie uma função que gere um modelo baseado na estrutura do modelo de 3 camadas ocultas que melhor se adeque para a base de dados (de acordo com a teoria), crie e treine por 50 épocas um modelo gerado com esta função;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgUk-oHrCDE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1758b2cf-517a-45bc-f484-e15428851078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 2s 5ms/step - loss: 166007728.0000 - val_loss: 16158123.0000\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13309703.0000 - val_loss: 10986792.0000\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 10816167.0000 - val_loss: 10108262.0000\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 10071503.0000 - val_loss: 9877245.0000\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 9708851.0000 - val_loss: 9465374.0000\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 9382998.0000 - val_loss: 9239047.0000\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 9138848.0000 - val_loss: 9125947.0000\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 8966202.0000 - val_loss: 8976167.0000\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8765682.0000 - val_loss: 8917200.0000\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8664052.0000 - val_loss: 8786805.0000\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8552899.0000 - val_loss: 8695677.0000\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8466815.0000 - val_loss: 8574280.0000\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 8388543.0000 - val_loss: 8488322.0000\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 8339351.5000 - val_loss: 8417337.0000\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 8229800.5000 - val_loss: 8354797.0000\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8158991.0000 - val_loss: 8373968.0000\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8113611.0000 - val_loss: 8191273.5000\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8051546.0000 - val_loss: 8118786.0000\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7955490.5000 - val_loss: 8202263.5000\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7914641.5000 - val_loss: 8125113.0000\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7827556.0000 - val_loss: 7951514.5000\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7778819.5000 - val_loss: 7963489.0000\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7694831.5000 - val_loss: 7857473.5000\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7639016.0000 - val_loss: 7870234.0000\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7594919.5000 - val_loss: 7785503.5000\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7530973.0000 - val_loss: 7711959.0000\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7482939.0000 - val_loss: 7633776.0000\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7407826.5000 - val_loss: 7516158.0000\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7382272.0000 - val_loss: 7433173.5000\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 7299038.5000 - val_loss: 7412743.5000\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 7259860.5000 - val_loss: 7409587.5000\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7210851.0000 - val_loss: 7314727.5000\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7146769.0000 - val_loss: 7181554.5000\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7112532.5000 - val_loss: 7256201.5000\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7091592.5000 - val_loss: 7214082.0000\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7063140.0000 - val_loss: 7159188.5000\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7025427.5000 - val_loss: 7075280.0000\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6984247.0000 - val_loss: 7080430.5000\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 6954654.0000 - val_loss: 7026924.0000\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 6916603.5000 - val_loss: 7011417.0000\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 6908252.5000 - val_loss: 6947285.5000\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 6870139.0000 - val_loss: 6906066.5000\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 6856420.5000 - val_loss: 7196782.0000\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 6827284.0000 - val_loss: 6844827.0000\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 6805691.0000 - val_loss: 6871468.5000\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 6812099.5000 - val_loss: 6755574.0000\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 6768090.5000 - val_loss: 6820093.0000\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 6745579.5000 - val_loss: 6833546.0000\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 6760124.5000 - val_loss: 6746294.0000\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 6745197.5000 - val_loss: 6778847.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x797c4e463070>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "X = data.drop('valor', axis=1)\n",
        "y = data['valor']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "def create_regularized_model(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "    return model\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = 1\n",
        "model = create_regularized_model(input_dim, output_dim)\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwc7f0O0CDE5"
      },
      "source": [
        "\n",
        "## 5 - De acordo com as peculiaridades de aplicação da aplicação do dropout, crie uma função que gere um modelo baseado na estrutura do modelo de 3 camadas ocultas que melhor se adeque para a base de dados (de acordo com a teoria), em seguida crie e treine por 50 épocas um modelo gerado com esta função;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTBvMuYYCDE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3831ddae-30ce-4d2a-9211-11fab2ec1db6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 3s 4ms/step - loss: 168589328.0000 - val_loss: 17183814.0000\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 18810396.0000 - val_loss: 11369981.0000\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 16232132.0000 - val_loss: 10211966.0000\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 15314963.0000 - val_loss: 9888049.0000\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 14447935.0000 - val_loss: 9625206.0000\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 14132345.0000 - val_loss: 9862545.0000\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13667872.0000 - val_loss: 9228351.0000\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13555498.0000 - val_loss: 9187781.0000\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13394670.0000 - val_loss: 8975849.0000\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13111468.0000 - val_loss: 8876063.0000\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12870890.0000 - val_loss: 8770020.0000\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13026195.0000 - val_loss: 8631798.0000\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13000779.0000 - val_loss: 8770829.0000\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13028859.0000 - val_loss: 8475862.0000\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 12925352.0000 - val_loss: 8575756.0000\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12637869.0000 - val_loss: 8380193.5000\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12680033.0000 - val_loss: 8506716.0000\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12413678.0000 - val_loss: 8283448.5000\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12719560.0000 - val_loss: 8348175.5000\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12539124.0000 - val_loss: 8255443.5000\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12577646.0000 - val_loss: 8400919.0000\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12110608.0000 - val_loss: 8243999.0000\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12602986.0000 - val_loss: 8343842.0000\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12152376.0000 - val_loss: 8113393.5000\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12159702.0000 - val_loss: 8142157.0000\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12143159.0000 - val_loss: 8201096.5000\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12218887.0000 - val_loss: 8320311.5000\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12355558.0000 - val_loss: 8281906.5000\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12136851.0000 - val_loss: 8040057.5000\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12154352.0000 - val_loss: 7977436.5000\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12024488.0000 - val_loss: 8225138.0000\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12114312.0000 - val_loss: 8269831.0000\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 12164844.0000 - val_loss: 8042648.5000\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12215562.0000 - val_loss: 8117891.0000\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12185828.0000 - val_loss: 7986597.5000\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12194682.0000 - val_loss: 7944675.5000\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12213417.0000 - val_loss: 7891810.5000\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 11996627.0000 - val_loss: 7960992.5000\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12049347.0000 - val_loss: 7940973.0000\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 11877167.0000 - val_loss: 8093755.5000\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 11659767.0000 - val_loss: 7923952.5000\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 11576308.0000 - val_loss: 8128410.0000\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 11849157.0000 - val_loss: 7880576.0000\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 11907810.0000 - val_loss: 7791910.0000\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 11771124.0000 - val_loss: 7898698.0000\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 11706490.0000 - val_loss: 8377743.0000\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12050503.0000 - val_loss: 7936267.0000\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 11792447.0000 - val_loss: 7766106.0000\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 11948487.0000 - val_loss: 7905921.0000\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 11731970.0000 - val_loss: 8130457.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x797c4e474f10>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "X = data.drop('valor', axis=1)\n",
        "y = data['valor']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "def create_model_with_dropout(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "    return model\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = 1\n",
        "model = create_model_with_dropout(input_dim, output_dim)\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN204fe9CDE5"
      },
      "source": [
        "\n",
        "## 7 - De acordo com as peculiaridades de aplicação da regularização de kernel L1, L2 e Elastic Net (L1 e L2 Juntos) e da aplicação do dropout, crie uma função que gere um modelo  baseado na estrutura do modelo de 3 camadas ocultas que melhor se adeque para a base de dados (basicamente mesclar os dois modelos anteriores), em seguida crie e treine por 50 épocas um modelo gerado com esta função;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UOAghU8CDE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffec2d49-69b5-41fd-e85e-bd9bda1a3095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 2s 4ms/step - loss: 180989808.0000 - val_loss: 18512798.0000\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 19562974.0000 - val_loss: 11670758.0000\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 16497494.0000 - val_loss: 10644531.0000\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 15703654.0000 - val_loss: 9903884.0000\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 15365277.0000 - val_loss: 9751057.0000\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 15182068.0000 - val_loss: 9444759.0000\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 14610156.0000 - val_loss: 9555328.0000\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 14637834.0000 - val_loss: 9091844.0000\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 14157518.0000 - val_loss: 8976734.0000\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13924910.0000 - val_loss: 9169775.0000\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13935082.0000 - val_loss: 8814676.0000\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13964323.0000 - val_loss: 8753161.0000\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13796867.0000 - val_loss: 8655475.0000\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13831354.0000 - val_loss: 8616839.0000\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13387135.0000 - val_loss: 8465778.0000\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13476641.0000 - val_loss: 8579613.0000\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13441357.0000 - val_loss: 8409604.0000\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13466027.0000 - val_loss: 8402828.0000\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13412660.0000 - val_loss: 8531964.0000\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13508784.0000 - val_loss: 8259295.5000\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13550129.0000 - val_loss: 8648187.0000\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 13134367.0000 - val_loss: 8465077.0000\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13228685.0000 - val_loss: 8473159.0000\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13239252.0000 - val_loss: 8562667.0000\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13147071.0000 - val_loss: 8261232.0000\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13196171.0000 - val_loss: 8176090.5000\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13075992.0000 - val_loss: 8103236.0000\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13015745.0000 - val_loss: 8603506.0000\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12976605.0000 - val_loss: 8094663.5000\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13002544.0000 - val_loss: 8196323.5000\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13126863.0000 - val_loss: 8178386.5000\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12626268.0000 - val_loss: 8639798.0000\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12639105.0000 - val_loss: 8059048.5000\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12943600.0000 - val_loss: 8185885.5000\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 12957744.0000 - val_loss: 8114969.5000\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 2s 10ms/step - loss: 12622063.0000 - val_loss: 7940911.5000\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 12763334.0000 - val_loss: 8060469.0000\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 12778085.0000 - val_loss: 7919592.5000\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 12741948.0000 - val_loss: 8072809.0000\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12447741.0000 - val_loss: 7933088.5000\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12554028.0000 - val_loss: 8365901.5000\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 12231705.0000 - val_loss: 7881351.5000\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 12498060.0000 - val_loss: 8179450.0000\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 12241689.0000 - val_loss: 8481471.0000\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 12479394.0000 - val_loss: 8025624.0000\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12380741.0000 - val_loss: 7890766.5000\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12786077.0000 - val_loss: 8295096.5000\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12357635.0000 - val_loss: 8032177.0000\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12577167.0000 - val_loss: 7861615.0000\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12407231.0000 - val_loss: 8238033.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x797c4c5a6aa0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X = data.drop('valor', axis=1)\n",
        "y = data['valor']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "def create_combined_model(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "    return model\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = 1\n",
        "model = create_combined_model(input_dim, output_dim)\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8U55ZmyCDE5"
      },
      "source": [
        "\n",
        "## 8 - Crie e treine por 50 épocas um modelo gerado com a função create_7_layer_model e outro com a função create_3_layer_model;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DG2FlQlCDE6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1afddbf9-5b2e-430a-bb15-f534aae5a431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 5s 4ms/step - loss: 104826960.0000 - val_loss: 12468696.0000\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 19768440.0000 - val_loss: 12502645.0000\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 18075334.0000 - val_loss: 9690302.0000\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 17460874.0000 - val_loss: 9559293.0000\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 16543303.0000 - val_loss: 9075967.0000\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 16081832.0000 - val_loss: 9224269.0000\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 15632420.0000 - val_loss: 8670029.0000\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 15115043.0000 - val_loss: 9009706.0000\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 15105305.0000 - val_loss: 8714178.0000\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 15394834.0000 - val_loss: 10088948.0000\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 14815242.0000 - val_loss: 9472219.0000\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14516964.0000 - val_loss: 8594651.0000\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14387053.0000 - val_loss: 9174523.0000\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 14507355.0000 - val_loss: 8692640.0000\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14801004.0000 - val_loss: 8940659.0000\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 14462050.0000 - val_loss: 8113114.0000\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14040951.0000 - val_loss: 8769861.0000\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13936195.0000 - val_loss: 8852051.0000\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 14418482.0000 - val_loss: 8114076.5000\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13893655.0000 - val_loss: 8990670.0000\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13931076.0000 - val_loss: 8308646.0000\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13775587.0000 - val_loss: 8712077.0000\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 14149543.0000 - val_loss: 9529034.0000\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 13840142.0000 - val_loss: 8691159.0000\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13751742.0000 - val_loss: 8786938.0000\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13918111.0000 - val_loss: 7930070.5000\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13850853.0000 - val_loss: 9001778.0000\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13544988.0000 - val_loss: 8289573.0000\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 13279116.0000 - val_loss: 8690391.0000\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 13701875.0000 - val_loss: 8847035.0000\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 13685521.0000 - val_loss: 8118629.5000\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13297803.0000 - val_loss: 9143117.0000\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13462666.0000 - val_loss: 8825934.0000\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 13440092.0000 - val_loss: 8195776.0000\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 13631238.0000 - val_loss: 9872442.0000\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 13423323.0000 - val_loss: 9260976.0000\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13445039.0000 - val_loss: 8025360.5000\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13661009.0000 - val_loss: 8728063.0000\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13504416.0000 - val_loss: 7779807.0000\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12994791.0000 - val_loss: 10155098.0000\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13081297.0000 - val_loss: 7970749.5000\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12705525.0000 - val_loss: 10288984.0000\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 13194893.0000 - val_loss: 8900354.0000\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 12869204.0000 - val_loss: 9125787.0000\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 12522736.0000 - val_loss: 8404191.0000\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 12755677.0000 - val_loss: 8816350.0000\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 13039321.0000 - val_loss: 9701616.0000\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 12935703.0000 - val_loss: 9285577.0000\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 12784936.0000 - val_loss: 10956726.0000\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 12838283.0000 - val_loss: 11649015.0000\n",
            "Epoch 1/50\n",
            "250/250 [==============================] - 3s 7ms/step - loss: 169031024.0000 - val_loss: 16334851.0000\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 13489116.0000 - val_loss: 11237912.0000\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 10776157.0000 - val_loss: 10163516.0000\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 10104451.0000 - val_loss: 9760176.0000\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 9727106.0000 - val_loss: 9500745.0000\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 9446863.0000 - val_loss: 9302337.0000\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 9186266.0000 - val_loss: 9038715.0000\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 8957487.0000 - val_loss: 8876070.0000\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 8788903.0000 - val_loss: 8863697.0000\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 8618953.0000 - val_loss: 8551644.0000\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 8504679.0000 - val_loss: 8559940.0000\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 8412649.0000 - val_loss: 8548187.0000\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 8354282.0000 - val_loss: 8430270.0000\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 8281728.0000 - val_loss: 8331703.0000\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 8205146.0000 - val_loss: 8310847.5000\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 3s 10ms/step - loss: 8161344.0000 - val_loss: 8427926.0000\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 8101395.0000 - val_loss: 8169867.5000\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 8086331.5000 - val_loss: 8114407.5000\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 8026952.0000 - val_loss: 8250538.5000\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 8019045.0000 - val_loss: 8080812.0000\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7936082.5000 - val_loss: 8063082.5000\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 7898692.0000 - val_loss: 8386725.5000\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7845846.0000 - val_loss: 7949305.0000\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 7845488.0000 - val_loss: 7982524.5000\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7826374.0000 - val_loss: 7994568.0000\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7776135.5000 - val_loss: 7944657.0000\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7731096.5000 - val_loss: 8034901.0000\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7705066.0000 - val_loss: 7913063.0000\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7728759.5000 - val_loss: 7883032.5000\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7675878.5000 - val_loss: 7781218.0000\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7652927.0000 - val_loss: 7757808.5000\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 7623990.0000 - val_loss: 7938215.5000\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 7625259.5000 - val_loss: 7889135.0000\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7573117.0000 - val_loss: 7894871.5000\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7581032.5000 - val_loss: 7777498.0000\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7556450.5000 - val_loss: 7713496.5000\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7524727.5000 - val_loss: 7747764.5000\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7510078.5000 - val_loss: 7744768.5000\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7520863.5000 - val_loss: 7784644.5000\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7486474.0000 - val_loss: 7623023.5000\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7482536.5000 - val_loss: 7733439.5000\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7445401.0000 - val_loss: 7660275.0000\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7513070.5000 - val_loss: 7874221.5000\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7436631.0000 - val_loss: 7679536.5000\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7459982.0000 - val_loss: 7659576.5000\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7433856.5000 - val_loss: 7711672.0000\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7431757.0000 - val_loss: 7613610.0000\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7402863.0000 - val_loss: 7556223.5000\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 7391169.5000 - val_loss: 7606416.0000\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 7410866.0000 - val_loss: 7579319.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x797c4d61e320>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X = data.drop('valor', axis=1)\n",
        "y = data['valor']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "def create_7_layer_model(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "    return model\n",
        "\n",
        "def create_3_layer_model(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "    return model\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = 1\n",
        "\n",
        "model_7_layer = create_7_layer_model(input_dim, output_dim)\n",
        "\n",
        "model_7_layer.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "model_7_layer.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
        "\n",
        "model_3_layer = create_3_layer_model(input_dim, output_dim)\n",
        "\n",
        "model_3_layer.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
        "\n",
        "model_3_layer.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEfazjQWCDE6"
      },
      "source": [
        "\n",
        "## 9 - Faça previsão com os 8 modelos nos dados de teste exibindo um gráfico com o MSE de cada um deles;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G69YElhhCDE6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0fef639b-2694-437c-fc43-31728745378d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "250/250 [==============================] - 8s 4ms/step - loss: 86624016.0000 - val_loss: 12321677.0000\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 19220162.0000 - val_loss: 12703560.0000\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 18090844.0000 - val_loss: 9775073.0000\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 16737691.0000 - val_loss: 10405581.0000\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 16292939.0000 - val_loss: 9406188.0000\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 16008218.0000 - val_loss: 9516080.0000\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 15328191.0000 - val_loss: 8769592.0000\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 15276460.0000 - val_loss: 9226942.0000\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 15300755.0000 - val_loss: 9173873.0000\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 14984089.0000 - val_loss: 8576320.0000\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 14759955.0000 - val_loss: 8907677.0000\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 14586283.0000 - val_loss: 8595297.0000\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 14517160.0000 - val_loss: 8291138.5000\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 14453188.0000 - val_loss: 8390889.0000\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 2s 6ms/step - loss: 14513608.0000 - val_loss: 8430033.0000\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 13847868.0000 - val_loss: 8250217.5000\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 13966961.0000 - val_loss: 8124172.5000\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 2s 8ms/step - loss: 13757608.0000 - val_loss: 8041467.5000\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 2s 9ms/step - loss: 13817985.0000 - val_loss: 8322450.0000\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 2s 7ms/step - loss: 14119146.0000 - val_loss: 8381393.5000\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13835696.0000 - val_loss: 8197181.0000\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13681487.0000 - val_loss: 9506727.0000\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13349335.0000 - val_loss: 8088018.0000\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13694332.0000 - val_loss: 8632139.0000\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 14014091.0000 - val_loss: 8511295.0000\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13721983.0000 - val_loss: 8016919.5000\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13382871.0000 - val_loss: 8484477.0000\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13372177.0000 - val_loss: 8819757.0000\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13638360.0000 - val_loss: 8482809.0000\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 13462858.0000 - val_loss: 8371179.0000\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 13556522.0000 - val_loss: 8215849.5000\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13277535.0000 - val_loss: 7987433.5000\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13242820.0000 - val_loss: 9792628.0000\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13204321.0000 - val_loss: 7858626.0000\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13298494.0000 - val_loss: 8128461.5000\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13649470.0000 - val_loss: 8264989.5000\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 13111434.0000 - val_loss: 9640997.0000\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13213722.0000 - val_loss: 9353511.0000\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 13169662.0000 - val_loss: 8972970.0000\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 1s 6ms/step - loss: 13004092.0000 - val_loss: 7746570.0000\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 1s 5ms/step - loss: 13407827.0000 - val_loss: 7742980.5000\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 3s 12ms/step - loss: 13084101.0000 - val_loss: 8104039.0000\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12885317.0000 - val_loss: 8665609.0000\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12983291.0000 - val_loss: 9973992.0000\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12900776.0000 - val_loss: 9700568.0000\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12822110.0000 - val_loss: 7959317.5000\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12858358.0000 - val_loss: 8215501.0000\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 13010346.0000 - val_loss: 8396334.0000\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 12899035.0000 - val_loss: 9060444.0000\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 12811870.0000 - val_loss: 7946096.0000\n",
            "Epoch 1/50\n",
            "250/250 [==============================] - 2s 5ms/step - loss: 169940160.0000 - val_loss: 17339982.0000\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 14054470.0000 - val_loss: 11175381.0000\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 10898013.0000 - val_loss: 10080258.0000\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 10111436.0000 - val_loss: 9613305.0000\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 9704859.0000 - val_loss: 9363401.0000\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 9415035.0000 - val_loss: 9227850.0000\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 9116401.0000 - val_loss: 9038964.0000\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8894623.0000 - val_loss: 8820692.0000\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8734833.0000 - val_loss: 8700933.0000\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8560926.0000 - val_loss: 8698747.0000\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8448893.0000 - val_loss: 8605096.0000\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8371727.5000 - val_loss: 8456963.0000\n",
            "Epoch 13/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8300490.0000 - val_loss: 8537727.0000\n",
            "Epoch 14/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8224195.5000 - val_loss: 8365026.0000\n",
            "Epoch 15/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 8140586.5000 - val_loss: 8376270.5000\n",
            "Epoch 16/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 8103425.0000 - val_loss: 8259397.5000\n",
            "Epoch 17/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 8017879.5000 - val_loss: 8454153.0000\n",
            "Epoch 18/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 7996594.0000 - val_loss: 8281979.0000\n",
            "Epoch 19/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7960007.5000 - val_loss: 8101115.0000\n",
            "Epoch 20/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 7888541.0000 - val_loss: 8273939.0000\n",
            "Epoch 21/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7871661.5000 - val_loss: 8020969.0000\n",
            "Epoch 22/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7826001.0000 - val_loss: 8007707.5000\n",
            "Epoch 23/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7793461.0000 - val_loss: 8029925.0000\n",
            "Epoch 24/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7742848.5000 - val_loss: 8028287.5000\n",
            "Epoch 25/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7708785.0000 - val_loss: 7899604.5000\n",
            "Epoch 26/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7680544.5000 - val_loss: 7881395.0000\n",
            "Epoch 27/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7673081.5000 - val_loss: 7969895.5000\n",
            "Epoch 28/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7668444.0000 - val_loss: 7991115.5000\n",
            "Epoch 29/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7632041.5000 - val_loss: 7840873.0000\n",
            "Epoch 30/50\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 7607424.5000 - val_loss: 7794383.0000\n",
            "Epoch 31/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7578429.0000 - val_loss: 7782445.0000\n",
            "Epoch 32/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7560569.0000 - val_loss: 7820182.0000\n",
            "Epoch 33/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7580638.0000 - val_loss: 7808106.5000\n",
            "Epoch 34/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 7543634.0000 - val_loss: 7781961.5000\n",
            "Epoch 35/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 7513642.0000 - val_loss: 7840311.0000\n",
            "Epoch 36/50\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 7526173.0000 - val_loss: 7840309.0000\n",
            "Epoch 37/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7502518.0000 - val_loss: 7813467.0000\n",
            "Epoch 38/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7485131.0000 - val_loss: 7769182.0000\n",
            "Epoch 39/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7484944.5000 - val_loss: 7756968.5000\n",
            "Epoch 40/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7430613.0000 - val_loss: 7684044.5000\n",
            "Epoch 41/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7427100.0000 - val_loss: 7724983.5000\n",
            "Epoch 42/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7432297.0000 - val_loss: 7752380.5000\n",
            "Epoch 43/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7439506.0000 - val_loss: 7715239.0000\n",
            "Epoch 44/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7401485.0000 - val_loss: 7678529.5000\n",
            "Epoch 45/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7418888.5000 - val_loss: 7647957.5000\n",
            "Epoch 46/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7404202.0000 - val_loss: 7681637.5000\n",
            "Epoch 47/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7373863.5000 - val_loss: 7644110.5000\n",
            "Epoch 48/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7364914.5000 - val_loss: 7757734.0000\n",
            "Epoch 49/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7368367.5000 - val_loss: 7672445.0000\n",
            "Epoch 50/50\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 7371811.5000 - val_loss: 7670360.0000\n",
            "63/63 [==============================] - 0s 2ms/step\n",
            "63/63 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqYAAAHWCAYAAAClsUvDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0bklEQVR4nO3deXQUVd7G8acJpAlkYQubdAiERUBA3BEBeQ1gjBgdxXkRNWwzKtsoihB9FRjA4IYM6qCDkMBowEHBhREBUWDYZA1oWGQLiaKAaNJsNpDc9w+PPTYJIdGQuoTv55w6nrp1q+pX5fH6pLZ2GWOMAAAAAIdVcLoAAAAAQCKYAgAAwBIEUwAAAFiBYAoAAAArEEwBAABgBYIpAAAArEAwBQAAgBUIpgAAALACwRQAAABWIJgCQAm5XC6NHj3a6TLKRHR0tPr06fOb1r2YzhOA0kEwBeCI1NRUuVwuuVwurVixosByY4w8Ho9cLpduvfXWgGVHjx7VqFGjdNlll6lq1aqqWbOmLr/8cv3lL3/R/v37/f1Gjx7t30dh03fffXfej7M0LF261F/zm2++WWifDh06yOVy6bLLLivj6gCg9FR0ugAAF7fKlSsrLS1NN9xwQ0D7smXL9PXXX8vtdge0nzp1Sp06ddL27duVmJioIUOG6OjRo8rIyFBaWpruuOMO1a9fP2CdKVOmKDQ0tMC+q1WrVurHcz79cq7uvffegPbMzEytWrVKlStXdqgyACgdBFMAjrrllls0Z84cTZ48WRUr/ndISktL05VXXqnvv/8+oP97772nTZs26a233tI999wTsOynn37SyZMnC+zjrrvuUq1atc7PAZShW265RR988IG+//77gONJS0tTnTp11LRpU/34448OVggAvw+38gE4qlevXjp8+LAWL17sbzt58qTeeeedAsFTknbv3i3p51vXZ6pcubLCw8NLrTafz6dHHnlEkZGRCgsL02233aavv/660L6bNm1SXFycwsPDFRoaqptuuklr1qwJ6HPq1CmNGTNGTZs2VeXKlVWzZk3dcMMNAcdelISEBLndbs2ZMyegPS0tTXfffbeCgoIKrHP69GmNHTtWMTExcrvdio6O1hNPPCGfzxfQzxijcePGqUGDBqpSpYq6dOmijIyMQuvIycnRww8/LI/HI7fbrSZNmujZZ59Vfn7+OY+hLM4TgAsXwRSAo6Kjo9W+fXvNmjXL37ZgwQLl5ubqf//3fwv0b9iwoSRp5syZMsYUax8//PCDvv/++4ApJyfnnOsNGDBAkyZNUrdu3TRhwgRVqlRJ8fHxBfplZGSoY8eO2rx5sx5//HE99dRT2rt3r2688UZ9/vnn/n6jR4/WmDFj1KVLF73yyit68sknFRUVpY0bNxbrOKpUqaKEhISAc7V582ZlZGQUGuJ/OYann35aV1xxhV566SV17txZycnJBc7t008/raeeekpt27bV888/r8aNG6tbt246duxYQL/jx4+rc+fOevPNN3X//fdr8uTJ6tChg5KSkjRs2LAi6y+r8wTgAmYAwAEpKSlGklm3bp155ZVXTFhYmDl+/LgxxpiePXuaLl26GGOMadiwoYmPj/evd/z4cdO8eXMjyTRs2ND06dPHTJs2zRw4cKDAPkaNGmUkFTo1b968yPrS09ONJDNw4MCA9nvuucdIMqNGjfK33X777SY4ONjs3r3b37Z//34TFhZmOnXq5G9r27ZtwLEU12effWYkmTlz5pj58+cbl8tlsrKyjDHGDB8+3DRu3NgYY0znzp1Nq1atChzDgAEDArb32GOPGUnm008/NcYYc/DgQRMcHGzi4+NNfn6+v98TTzxhJJnExER/29ixY03VqlXNV199FbDNkSNHmqCgIH9dxpgyP08ALnxcMQXguLvvvlsnTpzQ/PnzdeTIEc2fP/+sVwBDQkL0+eefa/jw4ZJ+fru/f//+qlevnoYMGVLgFrUkvfvuu1q8eHHAlJKSUmRNH330kSRp6NChAe0PP/xwwHxeXp4WLVqk22+/XY0bN/a316tXT/fcc49WrFghr9cr6eeXrTIyMrRz586iT0gRunXrpho1amj27Nkyxmj27Nnq1atXkcdw5pXMRx99VJL073//W5L0ySef6OTJkxoyZIhcLtdZj1WS5syZo44dO6p69eoBV6BjY2OVl5en5cuXF1pLWZ8nABemchNMly9frh49eqh+/fpyuVx67733SrwNY4xeeOEFNWvWTG63W5dcconGjx9f+sUCCBAZGanY2FilpaVp7ty5ysvL01133XXW/hEREXruueeUmZmpzMxMTZs2Tc2bN9crr7yisWPHFujfqVMnxcbGBkzt27cvsqZ9+/apQoUKiomJCWhv3rx5wPyhQ4d0/PjxAu2S1KJFC+Xn5ys7O1uS9Ne//lU5OTlq1qyZWrdureHDh2vLli1F1nGmSpUqqWfPnkpLS9Py5cuVnZ191hD/yzE0adIkoL1u3bqqVq2a9u3b5+8nSU2bNg3oFxkZqerVqwe07dy5Ux9//LEiIyMDptjYWEnSwYMHC62lrM8TgAtTuQmmx44dU9u2bfXqq6/+5m385S9/0RtvvKEXXnhB27dv1wcffKBrrrmmFKsEcDb33HOPFixYoNdee01xcXHF/pRTw4YN1a9fP61cuVLVqlXTW2+9dX4L/R06deqk3bt3a/r06brsssv0xhtv6IorrtAbb7xRou3cc889Sk9P1+jRo9W2bVu1bNmyyP6/vgr6e+Xn56tr164FrkD/Mt15552/ex+ldZ4AXHjKzeei4uLiFBcXd9blPp9PTz75pGbNmqWcnBxddtllevbZZ3XjjTdKkrZt26YpU6boyy+/9P9F36hRo7IoHYCkO+64Qw888IDWrFmjt99+u8TrV69eXTExMfryyy9LpZ6GDRsqPz9fu3fvDrjKt2PHjoB+kZGRqlKlSoF2Sdq+fbsqVKggj8fjb6tRo4b69u2rvn376ujRo+rUqZNGjx6tAQMGFLu2G264QVFRUVq6dKmeffbZcx7Dzp071aJFC3/7gQMHlJOT43+R7Jd/7ty5M+A2+6FDhwp8fiomJkZHjx71XyEtLifOE4ALT7m5YnougwcP1urVqzV79mxt2bJFPXv21M033+x/hunDDz9U48aNNX/+fDVq1EjR0dEaMGCAfvjhB4crBy4OoaGhmjJlikaPHq0ePXqctd/mzZsLfNtU+vl29NatWwu9Vfxb/PKH7uTJkwPaJ02aFDAfFBSkbt266f3331dmZqa//cCBA/4fDvjlE1aHDx8OWDc0NFRNmjQp9LnYorhcLk2ePFmjRo3Sfffdd9Z+t9xyS6E1T5w4UZL8XxiIjY1VpUqV9PLLLwd86eDM9aSfnwdevXq1Fi5cWGBZTk6OTp8+XWgtTpwnABeecnPFtChZWVlKSUlRVlaW/xdhHnvsMX388cdKSUnRM888oz179mjfvn2aM2eOZs6cqby8PD3yyCO666679Omnnzp8BMDFITEx8Zx9Fi9erFGjRum2227Tddddp9DQUO3Zs0fTp0+Xz+cr9LfZ33nnnUJ/+alr166qU6dOofu5/PLL1atXL/39739Xbm6urr/+ei1ZskS7du0q0HfcuHFavHixbrjhBg0cOFAVK1bU66+/Lp/Pp+eee87fr2XLlrrxxht15ZVXqkaNGlq/fr3eeecdDR48+JzHfaaEhAQlJCQU2adt27ZKTEzUP/7xD+Xk5Khz585au3atZsyYodtvv11dunSR9PPVzMcee0zJycm69dZbdcstt2jTpk1asGBBgR8mGD58uD744APdeuut6tOnj6688kodO3ZMX3zxhd555x1lZmae9ccMnDhPAC4wDn8V4LyQZObNm+efnz9/vpFkqlatGjBVrFjR3H333cYYY/70pz8ZSWbHjh3+9TZs2GAkme3bt5f1IQDl3q8/F1WUMz8XtWfPHvP000+b6667ztSuXdtUrFjRREZGmvj4eP/nj35R1OeiJJnPPvusyH2fOHHCDB061NSsWdNUrVrV9OjRw2RnZxf4DJIxxmzcuNF0797dhIaGmipVqpguXbqYVatWBfQZN26cueaaa0y1atVMSEiIufTSS8348ePNyZMni6zj15+LKsqZn4syxphTp06ZMWPGmEaNGplKlSoZj8djkpKSzE8//RTQLy8vz4wZM8bUq1fPhISEmBtvvNF8+eWXpmHDhgGfizLGmCNHjpikpCTTpEkTExwcbGrVqmWuv/5688ILLwQcS1mfJwAXPpcxxfxC9QXE5XJp3rx5uv322yVJb7/9tnr37q2MjIwCv4wSGhqqunXratSoUXrmmWd06tQp/7ITJ06oSpUqWrRokbp27VqWhwAAAHDRuShu5bdr1055eXk6ePCgOnbsWGifDh066PTp09q9e7f/8zBfffWVpP++GAAAAIDzp9xcMT169Kj/2a927dpp4sSJ6tKli2rUqKGoqCjde++9WrlypV588UW1a9dOhw4d0pIlS9SmTRvFx8crPz9fV199tUJDQzVp0iTl5+dr0KBBCg8P16JFixw+OgAAgPKv3ATTpUuX+h/k/7XExESlpqbq1KlTGjdunGbOnKlvvvlGtWrV0nXXXacxY8aodevWkqT9+/dryJAhWrRokapWraq4uDi9+OKLqlGjRlkfDgAAwEWn3ARTAAAAXNgumu+YAgAAwG4EUwAAAFjhgn4rPz8/X/v371dYWFip/hY0AAAASocxRkeOHFH9+vVVoULR10Qv6GC6f//+gN9WBgAAgJ2ys7PVoEGDIvtc0ME0LCxM0s8H+stvLAMAAMAeXq9XHo/Hn9uKckEH019u34eHhxNMAQAALFacxy55+QkAAABWIJgCAADACgRTAAAAWIFgCgAAACsQTAEAAGAFgikAAACsQDAFAACAFQimAAAAsALBFAAAAFZwNJjm5eXpqaeeUqNGjRQSEqKYmBiNHTtWxhgnywIAAIADHP1J0meffVZTpkzRjBkz1KpVK61fv159+/ZVRESEhg4d6mRpAAAAKGOOBtNVq1YpISFB8fHxkqTo6GjNmjVLa9eudbIsAAAAOMDRW/nXX3+9lixZoq+++kqStHnzZq1YsUJxcXGF9vf5fPJ6vQETAAAAygdHr5iOHDlSXq9Xl156qYKCgpSXl6fx48erd+/ehfZPTk7WmDFjyrhKAAAAlAVHr5j+61//0ltvvaW0tDRt3LhRM2bM0AsvvKAZM2YU2j8pKUm5ubn+KTs7u4wrBgAAwPniMg6+Au/xeDRy5EgNGjTI3zZu3Di9+eab2r59+znX93q9ioiIUG5ursLDw89nqX4uV5nsBoDD+DgIAJSOkuQ1R6+YHj9+XBUqBJYQFBSk/Px8hyoCAACAUxx9xrRHjx4aP368oqKi1KpVK23atEkTJ05Uv379nCwLAAAADnD0Vv6RI0f01FNPad68eTp48KDq16+vXr166emnn1ZwcPA51+dWPoDzhVv5AFA6SpLXHA2mvxfBFMD5cuGOjABglwvmGVMAAADgFwRTAAAAWIFgCgAAACsQTAEAAGAFgikAAACsQDAFAACAFQimAAAAsALBFAAAAFZw9CdJAQB24UdEgIuDrT8iwhVTAAAAWIFgCgAAACsQTAEAAGAFgikAAACsQDAFAACAFQimAAAAsALBFAAAAFYgmAIAAMAKBFMAAABYgWAKAAAAKxBMAQAAYAWCKQAAAKxAMAUAAIAVCKYAAACwAsEUAAAAViCYAgAAwAoEUwAAAFiBYAoAAAArEEwBAABgBYIpAAAArEAwBQAAgBUIpgAAALACwRQAAABWIJgCAADACgRTAAAAWIFgCgAAACs4Gkyjo6PlcrkKTIMGDXKyLAAAADigopM7X7dunfLy8vzzX375pbp27aqePXs6WBUAAACc4GgwjYyMDJifMGGCYmJi1LlzZ4cqAgAAgFMcDaa/dvLkSb355psaNmyYXC5XoX18Pp98Pp9/3uv1llV5AAAAOM+sefnpvffeU05Ojvr06XPWPsnJyYqIiPBPHo+n7AoEAADAeeUyxhini5Ck7t27Kzg4WB9++OFZ+xR2xdTj8Sg3N1fh4eFlUabOcjEXQDljx8hY9hjjgItDWY5xXq9XERERxcprVtzK37dvnz755BPNnTu3yH5ut1tut7uMqgIAAEBZsuJWfkpKimrXrq34+HinSwEAAIBDHA+m+fn5SklJUWJioipWtOICLgAAABzgeDD95JNPlJWVpX79+jldCgAAABzk+CXKbt26yZL3rwAAAOAgx6+YAgAAABLBFAAAAJYgmAIAAMAKBFMAAABYgWAKAAAAKxBMAQAAYAWCKQAAAKxAMAUAAIAVCKYAAACwAsEUAAAAViCYAgAAwAoEUwAAAFiBYAoAAAArEEwBAABgBYIpAAAArEAwBQAAgBUIpgAAALACwRQAAABWIJgCAADACgRTAAAAWIFgCgAAACsQTAEAAGAFgikAAACsQDAFAACAFQimAAAAsALBFAAAAFYgmAIAAMAKBFMAAABYgWAKAAAAKxBMAQAAYAWCKQAAAKxAMAUAAIAVCKYAAACwAsEUAAAAViCYAgAAwAqOB9NvvvlG9957r2rWrKmQkBC1bt1a69evd7osAAAAlLGKTu78xx9/VIcOHdSlSxctWLBAkZGR2rlzp6pXr+5kWQAAAHCAo8H02WeflcfjUUpKir+tUaNGDlYEAAAApzh6K/+DDz7QVVddpZ49e6p27dpq166dpk6detb+Pp9PXq83YAIAAED54Ggw3bNnj6ZMmaKmTZtq4cKFeuihhzR06FDNmDGj0P7JycmKiIjwTx6Pp4wrBgAAwPniMsYYp3YeHBysq666SqtWrfK3DR06VOvWrdPq1asL9Pf5fPL5fP55r9crj8ej3NxchYeHl0nNLleZ7AaAw5wbGZ3FGAdcHMpyjPN6vYqIiChWXnP0imm9evXUsmXLgLYWLVooKyur0P5ut1vh4eEBEwAAAMoHR4Nphw4dtGPHjoC2r776Sg0bNnSoIgAAADjF0WD6yCOPaM2aNXrmmWe0a9cupaWl6R//+IcGDRrkZFkAAABwgKPB9Oqrr9a8efM0a9YsXXbZZRo7dqwmTZqk3r17O1kWAAAAHODoy0+/V0kepi0tvBgAXBwu3JHx92GMAy4OvPwEAAAAFIFgCgAAACsQTAEAAGAFgikAAACsQDAFAACAFQimAAAAsALBFAAAAFYgmAIAAMAKBFMAAABYgWAKAAAAKxBMAQAAYAWCKQAAAKxAMAUAAIAVCKYAAACwAsEUAAAAViCYAgAAwAoEUwAAAFiBYAoAAAArEEwBAABgBYIpAAAArEAwBQAAgBUIpgAAALACwRQAAABWIJgCAADACgRTAAAAWIFgCgAAACsQTAEAAGAFgikAAACsQDAFAACAFQimAAAAsALBFAAAAFYgmAIAAMAKBFMAAABYgWAKAAAAKxBMAQAAYAVHg+no0aPlcrkCpksvvdTJkgAAAOCQik4X0KpVK33yySf++YoVHS8JAAAADnA8BVasWFF169Z1ugwAAAA4zPFnTHfu3Kn69eurcePG6t27t7Kyss7a1+fzyev1BkwAAAAoHxwNptdee61SU1P18ccfa8qUKdq7d686duyoI0eOFNo/OTlZERER/snj8ZRxxQAAADhfXMYY43QRv8jJyVHDhg01ceJE9e/fv8Byn88nn8/nn/d6vfJ4PMrNzVV4eHiZ1OhylcluADjMnpGxbDHGAReHshzjvF6vIiIiipXXHH/G9NeqVaumZs2aadeuXYUud7vdcrvdZVwVAAAAyoLjz5j+2tGjR7V7927Vq1fP6VIAAABQxhwNpo899piWLVumzMxMrVq1SnfccYeCgoLUq1cvJ8sCAACAAxy9lf/111+rV69eOnz4sCIjI3XDDTdozZo1ioyMdLIsAAAAOMDRYDp79mwndw8AAACLWPWMKQAAAC5eBFMAAABYgWAKAAAAKxBMAQAAYAWCKQAAAKxAMAUAAIAVCKYAAACwAsEUAAAAViCYAgAAwAoEUwAAAFiBYAoAAAArEEwBAABgBYIpAAAArEAwBQAAgBUIpgAAALACwRQAAABWIJgCAADACgRTAAAAWIFgCgAAACsQTAEAAGAFgikAAACsUKJg+txzz+nEiRP++ZUrV8rn8/nnjxw5ooEDB5ZedQAAALhouIwxpridg4KC9O2336p27dqSpPDwcKWnp6tx48aSpAMHDqh+/frKy8s7P9Wewev1KiIiQrm5uQoPDy+TfbpcZbIbAA4r/shYvjDGAReHshzjSpLXSnTF9MwMW4JMCwAAABSJZ0wBAABgBYIpAAAArFCxpCu88cYbCg0NlSSdPn1aqampqlWrlqSfX34CAAAAfosSvfwUHR0tVzGejN+7d+/vKqq4ePkJwPlysT5CzxgHXBxsffmpRFdMMzMzf09dAAAAwFnxjCkAAACsUKJgunr1as2fPz+gbebMmWrUqJFq166tP//5zwEf3AcAAACKq0TB9K9//asyMjL881988YX69++v2NhYjRw5Uh9++KGSk5NLvUgAAACUfyUKpunp6brpppv887Nnz9a1116rqVOnatiwYZo8ebL+9a9/lXqRAAAAKP9KFEx//PFH1alTxz+/bNkyxcXF+eevvvpqZWdnl151AAAAuGiUKJjWqVPH/ymokydPauPGjbruuuv8y48cOaJKlSqVboUAAAC4KJQomN5yyy0aOXKk/vOf/ygpKUlVqlRRx44d/cu3bNmimJiY31TIhAkT5HK59PDDD/+m9QEAAHBhK9F3TMeOHas//OEP6ty5s0JDQ5Wamqrg4GD/8unTp6tbt24lLmLdunV6/fXX1aZNmxKvCwAAgPKhRMG0Vq1aWr58uXJzcxUaGqqgoKCA5XPmzFFYWFiJCjh69Kh69+6tqVOnaty4cSVaFwAAAOVHiYJpv379itVv+vTpxd7moEGDFB8fr9jY2HMGU5/PF/CdVK/XW+z9AAAAwG4lCqapqalq2LCh2rVrJ1MKP7I6e/Zsbdy4UevWrStW/+TkZI0ZM+Z37xcAAAD2KVEwfeihhzRr1izt3btXffv21b333qsaNWr8ph1nZ2frL3/5ixYvXqzKlSsXa52kpCQNGzbMP+/1euXxeH7T/gEAAGAXlynhpU+fz6e5c+dq+vTpWrVqleLj49W/f39169ZNLper2Nt57733dMcddwQ8p5qXlyeXy6UKFSrI5/MVeIb1TF6vVxEREcrNzVV4eHhJDuM3K8EhAriAlcJNoQsSYxxwcSjLMa4kea3EwfTX9u3bp9TUVM2cOVOnT59WRkaGQkNDi7XukSNHtG/fvoC2vn376tJLL9WIESN02WWXnXMbBFMA5wvBFEB5ZmswLdGt/DNVqFBBLpdLxhjl5eWVaN2wsLAC4bNq1aqqWbNmsUIpAAAAypcSfWBf+vlW/qxZs9S1a1c1a9ZMX3zxhV555RVlZWUV+2opAAAAcKYSXTEdOHCgZs+eLY/Ho379+mnWrFmqVatWqRWzdOnSUtsWAAAALiwlesa0QoUKioqKUrt27Yp80Wnu3LmlUty58IwpgPOFZ0wBlGfl4hnT+++/v0Rv3gMAAADFVeIP7AMAAADnQ4lffgIAAADOB4IpAAAArEAwBQAAgBUIpgAAALACwRQAAABWIJgCAADACgRTAAAAWIFgCgAAACsQTAEAAGAFgikAAACsQDAFAACAFQimAAAAsALBFAAAAFYgmAIAAMAKBFMAAABYgWAKAAAAKxBMAQAAYAWCKQAAAKxAMAUAAIAVCKYAAACwAsEUAAAAViCYAgAAwAoEUwAAAFiBYAoAAAArEEwBAABgBYIpAAAArEAwBQAAgBUIpgAAALACwRQAAABWIJgCAADACgRTAAAAWIFgCgAAACs4GkynTJmiNm3aKDw8XOHh4Wrfvr0WLFjgZEkAAABwiKPBtEGDBpowYYI2bNig9evX63/+53+UkJCgjIwMJ8sCAACAA1zGGON0Eb9Wo0YNPf/88+rfv/85+3q9XkVERCg3N1fh4eFlUJ3kcpXJbgA4zK6RsewwxgEXh7Ic40qS1yqWUU3nlJeXpzlz5ujYsWNq3759oX18Pp98Pp9/3uv1llV5AAAAOM8cf/npiy++UGhoqNxutx588EHNmzdPLVu2LLRvcnKyIiIi/JPH4ynjagEAAHC+OH4r/+TJk8rKylJubq7eeecdvfHGG1q2bFmh4bSwK6Yej4db+QBKHbfyAZRntt7KdzyYnik2NlYxMTF6/fXXz9mXZ0wBnC92jYxlhzEOuDjYGkwdv5V/pvz8/ICrogAAALg4OPryU1JSkuLi4hQVFaUjR44oLS1NS5cu1cKFC50sCwAAAA5wNJgePHhQ999/v7799ltFRESoTZs2Wrhwobp27epkWQAAAHCAo8F02rRpTu4eAAAAFrHuGVMAAABcnAimAAAAsALBFAAAAFYgmAIAAMAKBFMAAABYgWAKAAAAKxBMAQAAYAWCKQAAAKxAMAUAAIAVCKYAAACwAsEUAAAAViCYAgAAwAoEUwAAAFiBYAoAAAArEEwBAABgBYIpAAAArEAwBQAAgBUIpgAAALACwRQAAABWIJgCAADACgRTAAAAWIFgCgAAACsQTAEAAGAFgikAAACsQDAFAACAFQimAAAAsALBFAAAAFYgmAIAAMAKBFMAAABYgWAKAAAAKxBMAQAAYAWCKQAAAKxAMAUAAIAVCKYAAACwAsEUAAAAViCYAgAAwAqOBtPk5GRdffXVCgsLU+3atXX77bdrx44dTpYEAAAAhzgaTJctW6ZBgwZpzZo1Wrx4sU6dOqVu3brp2LFjTpYFAAAAB7iMMcbpIn5x6NAh1a5dW8uWLVOnTp0KLPf5fPL5fP55r9crj8ej3NxchYeHl0mNLleZ7AaAw+wZGcsWYxxwcSjLMc7r9SoiIqJYec2qZ0xzc3MlSTVq1Ch0eXJysiIiIvyTx+Mpy/IAAABwHllzxTQ/P1+33XabcnJytGLFikL7cMUUQFmxY2Qse4xxwMXB1iumFcuopnMaNGiQvvzyy7OGUklyu91yu91lWBUAAADKihXBdPDgwZo/f76WL1+uBg0aOF0OAAAAHOBoMDXGaMiQIZo3b56WLl2qRo0aOVkOAAAAHORoMB00aJDS0tL0/vvvKywsTN99950kKSIiQiEhIU6WBgAAgDLm6MtPrrM8ZZ+SkqI+ffqcc/2SPExbWngxALg48PITgPKMl58KYckHAQAAAGABq75jCgAAgIsXwRQAAABWIJgCAADACgRTAAAAWIFgCgAAACsQTAEAAGAFgikAAACsQDAFAACAFQimAAAAsALBFAAAAFYgmAIAAMAKBFMAAABYgWAKAAAAKxBMAQAAYAWCKQAAAKxAMAUAAIAVCKYAAACwAsEUAAAAViCYAgAAwAoEUwAAAFiBYAoAAAArEEwBAABgBYIpAAAArEAwBQAAgBUIpgAAALACwRQAAABWIJgCAADACgRTAAAAWIFgCgAAACsQTAEAAGAFgikAAACsQDAFAACAFQimAAAAsALBFAAAAFYgmAIAAMAKjgbT5cuXq0ePHqpfv75cLpfee+89J8sBAACAgxwNpseOHVPbtm316quvOlkGAAAALFDRyZ3HxcUpLi7OyRIAAABgCUeDaUn5fD75fD7/vNfrdbAaAAAAlKYL6uWn5ORkRURE+CePx+N0SQAAACglF1QwTUpKUm5urn/Kzs52uiQAAACUkgvqVr7b7Zbb7Xa6DAAAAJwHF9QVUwAAAJRfjl4xPXr0qHbt2uWf37t3r9LT01WjRg1FRUU5WBkAAADKmqPBdP369erSpYt/ftiwYZKkxMREpaamOlQVAAAAnOBoML3xxhtljHGyBAAAAFiCZ0wBAABgBYIpAAAArEAwBQAAgBUIpgAAALACwRQAAABWIJgCAADACgRTAAAAWIFgCgAAACsQTAEAAGAFgikAAACsQDAFAACAFQimAAAAsALBFAAAAFYgmAIAAMAKBFMAAABYgWAKAAAAKxBMAQAAYAWCKQAAAKxAMAUAAIAVCKYAAACwAsEUAAAAViCYAgAAwAoEUwAAAFiBYAoAAAArEEwBAABgBYIpAAAArEAwBQAAgBUIpgAAALACwRQAAABWIJgCAADACgRTAAAAWIFgCgAAACsQTAEAAGAFgikAAACsQDAFAACAFawIpq+++qqio6NVuXJlXXvttVq7dq3TJQEAAKCMOR5M3377bQ0bNkyjRo3Sxo0b1bZtW3Xv3l0HDx50ujQAAACUIceD6cSJE/WnP/1Jffv2VcuWLfXaa6+pSpUqmj59utOlAQAAoAxVdHLnJ0+e1IYNG5SUlORvq1ChgmJjY7V69eoC/X0+n3w+n38+NzdXkuT1es9/sQAuKgwrAMqzshzjfslpxphz9nU0mH7//ffKy8tTnTp1Atrr1Kmj7du3F+ifnJysMWPGFGj3eDznrUYAF6eICKcrAIDzx4kx7siRI4o4x44dDaYllZSUpGHDhvnn8/Pz9cMPP6hmzZpyuVwOVobyzOv1yuPxKDs7W+Hh4U6XAwClijEO55sxRkeOHFH9+vXP2dfRYFqrVi0FBQXpwIEDAe0HDhxQ3bp1C/R3u91yu90BbdWqVTufJQJ+4eHhDNoAyi3GOJxP57pS+gtHX34KDg7WlVdeqSVLlvjb8vPztWTJErVv397BygAAAFDWHL+VP2zYMCUmJuqqq67SNddco0mTJunYsWPq27ev06UBAACgDDkeTP/4xz/q0KFDevrpp/Xdd9/p8ssv18cff1zghSjAKW63W6NGjSrwGAkAlAeMcbCJyxTn3X0AAADgPHP8A/sAAACARDAFAACAJQimAAAAsALBFBe9pUuXyuVyKScnx+lSfrPU1FS+6QugUIxxuJAQTHFO0dHRcrlcBaZBgwY5XRrOYvTo0br88sudLgO4IEyZMkVt2rTxf2C+ffv2WrBggdNloQiMceWX45+Lgv3WrVunvLw8//yXX36prl27qmfPng5WBQClo0GDBpowYYKaNm0qY4xmzJihhIQEbdq0Sa1atXK6POCiwhVTnFNkZKTq1q3rn+bPn6+YmBh17ty5yPWmT5+uVq1aye12q169eho8eLB/2cSJE9W6dWtVrVpVHo9HAwcO1NGjR/3Lf7ltM3/+fDVv3lxVqlTRXXfdpePHj2vGjBmKjo5W9erVNXTo0IDQ/M9//lNXXXWVwsLCVLduXd1zzz06ePBgQF0fffSRmjVrppCQEHXp0kWZmZkByw8fPqxevXrpkksuUZUqVdS6dWvNmjUroM8777yj1q1bKyQkRDVr1lRsbKyOHTt21nOxbNkyXXPNNf5zMXLkSJ0+fdq/PD8/X88995yaNGkit9utqKgojR8/XlLht+HS09PlcrkK1P7LuRszZow2b97sv7qdmpparPO+b98+9ejRQ9WrV1fVqlXVqlUrffTRR2c9LqA86NGjh2655RY1bdpUzZo10/jx4xUaGqo1a9YUuR5j3H8xxqHUGKAEfD6fqVmzphk/fnyR/f7+97+bypUrm0mTJpkdO3aYtWvXmpdeesm//KWXXjKffvqp2bt3r1myZIlp3ry5eeihh/zLU1JSTKVKlUzXrl3Nxo0bzbJly0zNmjVNt27dzN13320yMjLMhx9+aIKDg83s2bP9602bNs189NFHZvfu3Wb16tWmffv2Ji4uzr88KyvLuN1uM2zYMLN9+3bz5ptvmjp16hhJ5scffzTGGPP111+b559/3mzatMns3r3bTJ482QQFBZnPP//cGGPM/v37TcWKFc3EiRPN3r17zZYtW8yrr75qjhw5Uui5+Prrr02VKlXMwIEDzbZt28y8efNMrVq1zKhRo/x9Hn/8cVO9enWTmppqdu3aZf7zn/+YqVOnGmOM+eyzzwLqM8aYTZs2GUlm7969/vMVERFhjDHm+PHj5tFHHzWtWrUy3377rfn222/N8ePHi3Xe4+PjTdeuXc2WLVvM7t27zYcffmiWLVtW5L9roDw5ffq0mTVrlgkODjYZGRln7ccY91+McShNBFOUyNtvv22CgoLMN998U2S/+vXrmyeffLLY250zZ46pWbOmfz4lJcVIMrt27fK3PfDAA6ZKlSoBg2P37t3NAw88cNbtrlu3zkjyr5OUlGRatmwZ0GfEiBEFBsUzxcfHm0cffdQYY8yGDRuMJJOZmVmsY3viiSdM8+bNTX5+vr/t1VdfNaGhoSYvL894vV7jdrv9g/SZSjpoG2PMqFGjTNu2bc9Z25nnvXXr1mb06NHFOi6gPNmyZYupWrWqCQoKMhEREebf//53kf0Z4/6LMQ6liVv5KJFp06YpLi5O9evXP2ufgwcPav/+/brpppvO2ueTTz7RTTfdpEsuuURhYWG67777dPjwYR0/ftzfp0qVKoqJifHP16lTR9HR0QoNDQ1o+/VtrA0bNqhHjx6KiopSWFiY/3GDrKwsSdK2bdt07bXXBtTSvn37gPm8vDyNHTtWrVu3Vo0aNRQaGqqFCxf6t9G2bVvddNNNat26tXr27KmpU6fqxx9/POuxbtu2Te3bt5fL5fK3dejQQUePHtXXX3+tbdu2yefzFXm+Ssu5zvvQoUM1btw4dejQQaNGjdKWLVvOe02ADZo3b6709HR9/vnneuihh5SYmKitW7cW2pcxLhBjHEoTwRTFtm/fPn3yyScaMGBAkf1CQkKKXJ6Zmalbb71Vbdq00bvvvqsNGzbo1VdflSSdPHnS369SpUoB67lcrkLb8vPzJUnHjh1T9+7dFR4errfeekvr1q3TvHnzCmz3XJ5//nn97W9/04gRI/TZZ58pPT1d3bt3928jKChIixcv1oIFC9SyZUu9/PLLat68ufbu3Vvsffzauc5XhQo//2dqfvXrwadOnSrxfopz3gcMGKA9e/bovvvu0xdffKGrrrpKL7/8con3BVxogoOD1aRJE1155ZVKTk5W27Zt9be//a3QvoxxJcMYh5IgmKLYUlJSVLt2bcXHxxfZLywsTNHR0VqyZEmhyzds2KD8/Hy9+OKLuu6669SsWTPt37//d9e3fft2HT58WBMmTFDHjh116aWXFngpoEWLFlq7dm1A25kvOKxcuVIJCQm699571bZtWzVu3FhfffVVQB+Xy6UOHTpozJgx2rRpk4KDg/3/gzhTixYttHr16oBBd+XKlQoLC1ODBg3UtGlThYSEnPV8RUZGSpK+/fZbf1t6enqR5yI4ODjghQmp+Ofd4/HowQcf1Ny5c/Xoo49q6tSpRe4LKI/y8/Pl8/kKXcYYF4gxDqWJYIpiyc/PV0pKihITE1Wx4rm/MjZ69Gi9+OKLmjx5snbu3KmNGzf6/ypt0qSJTp06pZdffll79uzRP//5T7322mu/u8aoqCgFBwf7t/vBBx9o7NixAX0efPBB7dy5U8OHD9eOHTuUlpbmf5vzF02bNtXixYu1atUqbdu2TQ888IAOHDjgX/7555/rmWee0fr165WVlaW5c+fq0KFDatGiRaF1DRw4UNnZ2RoyZIi2b9+u999/X6NGjdKwYcNUoUIFVa5cWSNGjNDjjz+umTNnavfu3VqzZo2mTZvmP18ej0ejR4/Wzp079e9//1svvvhikeciOjpae/fuVXp6ur7//nv5fL5infeHH35YCxcu1N69e7Vx40Z99tlnZz0uoLxISkrS8uXLlZmZqS+++EJJSUlaunSpevfufdZ1GOP+izEOpcrZR1xxoVi4cKGRZHbs2FHsdV577TXTvHlzU6lSJVOvXj0zZMgQ/7KJEyeaevXqmZCQENO9e3czc+bMgIffz3zQ3ZjCH3ZPTEw0CQkJ/vm0tDQTHR1t3G63ad++vfnggw+MJLNp0yZ/nw8//NA0adLEuN1u07FjRzN9+vSAfR8+fNgkJCSY0NBQU7t2bfN///d/5v777/fvZ+vWraZ79+4mMjLSuN1u06xZM/Pyyy8XeS6WLl1qrr76ahMcHGzq1q1rRowYYU6dOuVfnpeXZ8aNG2caNmxoKlWqZKKioswzzzzjX75ixQrTunVrU7lyZdOxY0czZ86cIl8M+Omnn8ydd95pqlWrZiSZlJSUYp33wYMHm5iYGON2u01kZKS57777zPfff1/ksQEXun79+pmGDRua4OBgExkZaW666SazaNGic67HGPdfjHEoLS5jfnXtHQAAAHAIt/IBAABgBYIpAAAArEAwBQAAgBUIpgAAALACwRQAAABWIJgCAADACgRTAAAAWIFgCgAAACsQTAHAAkuXLpXL5VJOTk6x14mOjtakSZPOW00AUNYIpgBQDH369JHL5dKDDz5YYNmgQYPkcrnUp0+fsi8MAMoRgikAFJPH49Hs2bN14sQJf9tPP/2ktLQ0RUVFOVgZAJQPBFMAKKYrrrhCHo9Hc+fO9bfNnTtXUVFRateunb/N5/Np6NChql27tipXrqwbbrhB69atC9jWRx99pGbNmikkJERdunRRZmZmgf2tWLFCHTt2VEhIiDwej4YOHapjx46dtb6srCwlJCQoNDRU4eHhuvvuu3XgwAH/8s2bN6tLly4KCwtTeHi4rrzySq1fv/53nBEAKF0EUwAogX79+iklJcU/P336dPXt2zegz+OPP653331XM2bM0MaNG9WkSRN1795dP/zwgyQpOztbf/jDH9SjRw+lp6drwIABGjlyZMA2du/erZtvvll33nmntmzZorffflsrVqzQ4MGDC60rPz9fCQkJ+uGHH7Rs2TItXrxYe/bs0R//+Ed/n969e6tBgwZat26dNmzYoJEjR6pSpUqldWoA4PczAIBzSkxMNAkJCebgwYPG7XabzMxMk5mZaSpXrmwOHTpkEhISTGJiojl69KipVKmSeeutt/zrnjx50tSvX98899xzxhhjkpKSTMuWLQO2P2LECCPJ/Pjjj8YYY/r372/+/Oc/B/T5z3/+YypUqGBOnDhhjDGmYcOG5qWXXjLGGLNo0SITFBRksrKy/P0zMjKMJLN27VpjjDFhYWEmNTW1VM8LAJSmik4HYwC4kERGRio+Pl6pqakyxig+Pl61atXyL9+9e7dOnTqlDh06+NsqVaqka665Rtu2bZMkbdu2Tddee23Adtu3bx8wv3nzZm3ZskVvvfWWv80Yo/z8fO3du1ctWrQI6L9t2zZ5PB55PB5/W8uWLVWtWjVt27ZNV199tYYNG6YBAwbon//8p2JjY9WzZ0/FxMT8/pMCAKWEW/kAUEL9+vVTamqqZsyYoX79+p2XfRw9elQPPPCA0tPT/dPmzZu1c+fO3xwmR48erYyMDMXHx+vTTz9Vy5YtNW/evFKuHAB+O4IpAJTQzTffrJMnT+rUqVPq3r17wLKYmBgFBwdr5cqV/rZTp05p3bp1atmypSSpRYsWWrt2bcB6a9asCZi/4oortHXrVjVp0qTAFBwcXKCmFi1aKDs7W9nZ2f62rVu3Kicnx79fSWrWrJkeeeQRLVq0SH/4wx8CnpcFAKcRTAGghIKCgrRt2zZt3bpVQUFBAcuqVq2qhx56SMOHD9fHH3+srVu36k9/+pOOHz+u/v37S5IefPBB7dy5U8OHD9eOHTuUlpam1NTUgO2MGDFCq1at0uDBg5Wenq6dO3fq/fffP+vLT7GxsWrdurV69+6tjRs3au3atbr//vvVuXNnXXXVVTpx4oQGDx6spUuXat++fVq5cqXWrVtX4JEAAHASwRQAfoPw8HCFh4cXumzChAm68847dd999+mKK67Qrl27tHDhQlWvXl2SFBUVpXfffVfvvfee2rZtq9dee03PPPNMwDbatGmjZcuW6auvvlLHjh3Vrl07Pf3006pfv36h+3S5XHr//fdVvXp1derUSbGxsWrcuLHefvttST+H6cOHD+v+++9Xs2bNdPfddysuLk5jxowpxbMCAL+PyxhjnC4CAAAA4IopAAAArEAwBQAAgBUIpgAAALACwRQAAABWIJgCAADACgRTAAAAWIFgCgAAACsQTAEAAGAFgikAAACsQDAFAACAFQimAAAAsML/AwOQQOLI1rE0AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "X = data.drop('valor', axis=1)\n",
        "y = data['valor']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "def create_7_layer_model(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(16, activation='relu'))\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "    return model\n",
        "\n",
        "def create_3_layer_model(input_dim, output_dim):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(output_dim, activation='linear'))\n",
        "    return model\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = 1\n",
        "\n",
        "model_7_layer = create_7_layer_model(input_dim, output_dim)\n",
        "model_7_layer.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
        "model_7_layer.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
        "\n",
        "model_3_layer = create_3_layer_model(input_dim, output_dim)\n",
        "model_3_layer.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
        "model_3_layer.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
        "\n",
        "def calculate_mse(model, X, y):\n",
        "    y_pred = model.predict(X)\n",
        "    mse = mean_squared_error(y, y_pred)\n",
        "    return mse\n",
        "\n",
        "mse_7_layer = calculate_mse(model_7_layer, X_test, y_test)\n",
        "mse_3_layer = calculate_mse(model_3_layer, X_test, y_test)\n",
        "\n",
        "models = ['7 camadas ocultas', '3 camadas ocultas']\n",
        "mses = [mse_7_layer, mse_3_layer]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(models, mses, color='blue')\n",
        "plt.xlabel('Modelos')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('MSE dos Modelos')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vuDb3BcCDE6"
      },
      "source": [
        "\n",
        "## 10 - Selecione o melhor modelo e exiba as métricas (da função metricas_regressao) com os valores de teste e os valores de treinamento. Comparando as métricas, o modelo apresenta características de overfitting?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW-MSq0sCDE6"
      },
      "outputs": [],
      "source": [
        "def metricas_regressao(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return mse, mae, r2\n",
        "\n",
        "y_train_pred_7_layer = model_7_layer.predict(X_train)\n",
        "y_test_pred_7_layer = model_7_layer.predict(X_test)\n",
        "\n",
        "y_train_pred_3_layer = model_3_layer.predict(X_train)\n",
        "y_test_pred_3_layer = model_3_layer.predict(X_test)\n",
        "\n",
        "mse_train_7_layer, mae_train_7_layer, r2_train_7_layer = metricas_regressao(y_train, y_train_pred_7_layer)\n",
        "mse_test_7_layer, mae_test_7_layer, r2_test_7_layer = metricas_regressao(y_test, y_test_pred_7_layer)\n",
        "\n",
        "mse_train_3_layer, mae_train_3_layer, r2_train_3_layer = metricas_regressao(y_train, y_train_pred_3_layer)\n",
        "mse_test_3_layer, mae_test_3_layer, r2_test_3_layer = metricas_regressao(y_test, y_test_pred_3_layer)\n",
        "\n",
        "print(\"Métricas do Modelo com 7 Camadas Ocultas:\")\n",
        "print(f\"Treinamento - MSE: {mse_train_7_layer}, MAE: {mae_train_7_layer}, R²: {r2_train_7_layer}\")\n",
        "print(f\"Teste - MSE: {mse_test_7_layer}, MAE: {mae_test_7_layer}, R²: {r2_test_7_layer}\")\n",
        "print(\"\\n\")\n",
        "print(\"Métricas do Modelo com 3 Camadas Ocultas:\")\n",
        "print(f\"Treinamento - MSE: {mse_train_3_layer}, MAE: {mae_train_3_layer}, R²: {r2_train_3_layer}\")\n",
        "print(f\"Teste - MSE: {mse_test_3_layer}, MAE: {mae_test_3_layer}, R²: {r2_test_3_layer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "o Modelo com 7 Camadas Ocultas parece apresentar características de overfitting em comparação com o Modelo com 3 Camadas Ocultas, pois a diferença entre o desempenho no treinamento e no teste é maior no primeiro modelo. No entanto, a decisão final depende das métricas específicas e da importância relativa do desempenho no conjunto de teste para o problema específico."
      ],
      "metadata": {
        "id": "O8mnsFtO66Aw"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}